# Machine Learning Topics

## Data Processing

1. **Data Preparation**

2. **Data Imbalance Consequences**

3. **SMOTE**

4. **Feature Selection**

5. **Sampling**
   Process of selecting a subset from a larger group (called population) to make inferences about the whole.
   (Too costly and inefficient to test whole)

    1. Parameter vs Statistic

    - Parameter: True numeric value describing the population
    - Statistic: Numeric value derived from the sample.

    2. Sampling Techniques
        1. Probablity Sampling

        - Every member of population has a non zero chance of being selected.
            1. Simple Random Sampling

            - Every individual has equal chance of being selected
            - üü¢ Pros: Unbiased if true and easy to analyze
            - üî¥ Cons: Impractical due to requiring full list of population

            2. Stratified Sampling

            - Divide population in to homogenous subgroups (strata) and take random samples from each
              e.g: Divide by age
            - üü¢ Pros: Representation of all groups
            - üî¥ Cons: Requires complex design and knowledge to divide

            3. Systematic Sampling

            - Select every k-th item
            - üü¢ Pros: Easy, good coverage
            - üî¥ Cons: Hidden numerical patterns cause bias, also requires full list

            4. Cluster Sampling

            - Divide population into clusters and select one or more entire clusters
            - üü¢ Pros: Cheaper and convenient
            - üî¥ Cons: Less precise than stratified, bias due to only some clusters

            5. Mutli Stage Sampling

            - Combines Several Sampling Methods

        2. Non Probablity Sampling

        - Individuals have either unknown or unequal chance of being selection
            1. Convenience Sampling

            - Sample whoever you can
            - üü¢ Pros: Cheap and convenient
            - üî¥ Cons: Extremely Biased

            2. Purposive/Judgmental Sampling`‚ùå[Incomplete]`
            3. Quota Sampling`‚ùå[Incomplete]`
            4. Snowball Sampling`‚ùå[Incomplete]`
    3. Sampling Distributions

    - Probablity distribution of a sample statistic

    4. Central Limit Theorem

    - As sample size increases, sampling distribution of the mean tends to become normally distributed regardless of
      shape of population dist.
    - When sample is large (usually n>=30)
    - Sampling distribution should approximate normality to be valid

    5. Standard Error
       $$\text{SE} = \frac{s}{\sqrt{n}}$$

    - Standard deviation of sampling dist
    - Measures how much sample mean is expected to vary from sample to sample.
    - Decreases as n increases

    6. Law of large numbers

    - As number of observations increase sample mean converges with true population mean

    7. Sample size determination

    - For Population Mean
      $$n = \left( \frac{E}{Z \cdot \sigma} \right)^2$$
        - ùëõ = required sample size
        - ùëç = Z-score corresponding to the confidence level (e.g., 1.96 for 95%)
        - ùúé = population standard deviation (estimate if unknown)
        - ùê∏ = desired margin of error
    - For Population Proportion
      $$n = \frac{Z^2 \cdot p \cdot (1 - p)}{E^2}$$
        - Z = z-score corresponding to the desired confidence level
        - ùëù = estimated population proportion
        - ùê∏ = margin of error (in decimal form, e.g., 0.05 for 5%)

    8. Margin of Error (E)

    - Range within which true population parameter is expected to lie with level of confidence.


6. **Normalization**

## Text Preprocessing

1. ### Lowercasing
   - Standardize text to lowercase
2. ### Tokenization
   - Splits text into smaller units (called tokens), usually words.
   - Types of tokenization:
       - Word tokenization: Splits text into individual words.
       - Character tokenization: Splits text into characters.
           - üü¢ Small vocab
           - üî¥ Too much to compute
           - üî¥ Doesn't capture context
       - Sentence tokenization: Splits text into sentences. (very low usecase)
       - Subword tokenization: Breaks down into smaller units than words (useful in deep learning).  
         Two Popular Types: (‚ÑπÔ∏è Covered in LLM.md)
           1. Byte Pair Encoding
           2. Word Piece Encoding

3. 
4. ### Stopword Removal  
   -  Removes common words that appear frequently but add little meaning (e.g., "the", "is", "and").
   - üü¢ Reduces dimensionality and focuses on meaningful words.
5. ### Stemming and Lemmatization
   -  Stemming: Cuts words to their base form (may not be a real word). 
     - Example: "running" ‚Üí "run", "flies" ‚Üí "fli"
   - Lemmatization: Converts words to their meaningful base form using vocabulary and grammar.
     - o Example: "running" ‚Üí "run", "better" ‚Üí "good"
   - Lemmatization is usually more accurate than stemming.
6. Text Normalization
   ‚Ä¢ Standardizes text in various ways, such as:
   o Expanding contractions (e.g., "can't" ‚Üí "cannot")
   o Handling misspellings
   o Converting accented characters
   ‚Ä¢ Ensures consistency across the dataset.
7. ### Others:
   - Removing Punctuation, Numbers, and Special Characters
   - Removing Duplicates, Blank Lines
   - Spelling Correction
   - Slang and Abbreviation Handling (Optional)
   
8. ### Vectorization 
   - Converts processed text into numerical format.
   - Common methods:
   1. Bag of Words (BoW): Counts word frequency in each document. 
   2. TF-IDF (Term Frequency-Inverse Document Frequency): Weighs words by importance.
   3. Word Embeddings: Represents words in dense vector form based on meaning (e.g., Word2Vec, GloVe, BERT).

8. **Padding and Truncation of Text**
   Padding and truncation are important steps in text preprocessing when you're working with machine learning models
   that require fixed-length input, especially neural networks like RNNs, LSTMs, or transformers (like BERT).
    - Padding
        - What it does: Adds extra tokens (usually zeros or a special padding token) to the end or beginning of a
          sequence to make it a fixed length.
        - Why it's needed: Not all sentences are the same length, but models need inputs of the same size.
        - Where it's used: Often used after tokenization and before feeding data into a model.
    - Truncation
        - What it does: Cuts longer sequences down to the maximum allowed length.
        - Why it's needed: To avoid memory issues and keep computation efficient.
        - How it's done: Usually from the end (but can be from the start depending on the use case).

Common Settings (when using libraries like TensorFlow or Hugging Face):

- padding='max_length': Pads all sequences to the same length.
- truncation=True: Automatically truncates sequences longer than the max length.
- max_length=128: Defines the maximum sequence length.

11. **Word Embedding**
    - **Goal**: The goal behind embeddings is to convert words into vectors and ensure two words/vectors of similar
      meaning/context are mapped close in the embedding space.
      i.e Calculating the dot product (i.e angle between the two should be small) should be large.
    - **Word Embedding**: An embedding is a way to represent words as dense vectors in a lower-dimensional space, where
      similar words end up close together.
      Think of each dimension in the embedding as capturing some latent feature ‚Äî like "animal-ness", "femininity", "
      past tense-ness", etc.
      These features are not predefined by us ‚Äî the model learns them during training. It is like asking the model to
      classify words along N different (hidden) axes, where N is the embedding size.

    1. One Hot Encoding
    2. Distributed Representations
    3. Static Embeddings [DEPRECATED]
       Static embeddings are the oldest type of word embedding. The embeddings are generated against a large corpus but
       the number of words, though large, is finite. If you have a word whose embedding needs to be looked up that was
       not in the original corpus, then you are out of luck. In addition, a word has the same embedding regardless of
       how it is used, so static embeddings cannot address the problem of polysemy, that is, words with multiple
       meanings.
    4. Dynamic Embedding Dynamic embeddings are word representations that change depending on the context they appear
       in. This is in contrast to static embeddings, where each word has a single fixed vector, no matter where or how
       it appears.

       | Model |How it works |
       |----------------|-------------------------------|
       |ELMo|Uses a deep LSTM to generate embeddings based on the entire sentence|
       |BERT|Uses transformers and attention to create context-aware embeddings for each word|
       |GPT|Also produces dynamic embeddings (during generation or fine-tuning)|
    5. **Word2Vec**: turns words into dense vectors of numbers, so that similar words have similar vectors. These
       vectors capture semantic meaning ‚Äî for example:
       vector("king") - vector("man") + vector("woman") ‚âà vector("queen")
       Captures meaning and relationships between words. There are two main architectures:
        1. CBOW (Continuous Bag of Words): Predicts a word given its context (surrounding words).
           Example: "The cat sat on the ___" ‚Üí model predicts "mat".
           In Word2Vec (CBOW), the context window is the input to the model.

        2. Skip-Gram: Predicts context words from a given word.
           Example: Input "cat" ‚Üí model predicts words like "the", "sat", "on".
           In Skipgram we just pass one word

    **Characteristics of Word2Vec Embeddings***:
    1. Dense:
        - Unlike one-hot vectors (which are mostly 0s), Word2Vec embeddings are packed with real numbers.
        - Example: A one-hot vector for "cat" might be 10,000 dimensions long with only one "1". In contrast, a Word2Vec
          embedding might be 100-300 dimensions, all with meaningful values like [0.25, -1.3, 0.7, ..., 0.01].
    2. Low-dimensional:
        - Usually 50‚Äì300 dimensions, which is much smaller and more computationally efficient than sparse vectors.
    3. Semantic meaning is encoded:
        - Words that appear in similar contexts (e.g., ‚Äúking‚Äù and ‚Äúqueen‚Äù) will have similar vectors.
        - The relationships between vectors capture analogies (like we mentioned: king - man + woman ‚âà queen).
    4. Fixed size:
        - Each word, regardless of how common or rare, gets a vector of the same length.
    5. Context-independent:
        - Each word has one vector, no matter the sentence. So "bank" (as in river bank vs money bank) has the same
          embedding in both contexts.
        - This is a limitation that newer models like BERT try to solve.

    6. **Glove**
       GloVe differs from Word2Vec in that Word2Vec is a predictive model while GloVe is a count-based model. The first
       step is to construct a large matrix of (word, context) pairs that co-occur in the training corpus. Rows
       correspond to words and columns correspond to contexts, usually a sequence of one or more words. Each element of
       the matrix represents how often the word co-occurs in the context. The GloVe process factorizes this
       co-occurrence matrix into a pair of (word, feature) and (feature, context) matrices. The process is known as
       matrix factorization and is done using Stochastic Gradient Descent (SGD), an iterative numerical method. R = P *
       Q ‚âà R‚Äô
       Thus, the model decomposes a larger matrix R into it‚Äôs constituents approximately. The difference between the
       matrices R and R‚Äô represents the loss and is usually computed as the mean-squared error between the two matrices.
       The GloVe process is much more resource-intensive than Word2Vec. This is because Word2Vec learns the embedding by
       training over batches of word vectors, while GloVe factorizes the entire co-occurrence matrix in one shot.
    7. **Fastext**
    8. **Concept Number batch**

12. **Character Embedding**
    Another evolution of the basic word embedding strategy has been to look at character and subword embeddings instead
    of word embeddings. First, a character vocabulary is finite and small ‚Äì for example, a vocabulary for English would
    contain around 70 characters (26 characters, 10 numbers, and the rest special characters), leading to character
    models that are also small and compact. Second, unlike word embeddings, which provide vectors for a large but finite
    set of words, there is no concept of out-of-vocabulary for character embeddings, since any word can be represented
    by the vocabulary. Third, character embeddings tend to be better for rare and misspelled words because there is much
    less imbalance for character inputs than for word inputs. However, unlike word embeddings, character embeddings tend
    to be task specific and are usually generated inline within a network to support the task. For this reason, third
    party character embeddings are generally not available.
13. **Sentence Embedding**
14. **Language Based Model**
    A language model-based embedding is a type of contextualized vector representation for words, phrases, or entire
    sequences, generated by a neural language model that has been trained to understand natural language. Unlike static
    embeddings, which assign a single fixed vector to each word in the vocabulary, language model-based embeddings
    produce dynamic vectors that capture the semantic meaning of a word within its specific context. Language
    model-based embeddings are derived from deep neural architectures, often based on transformers (in models like BERT
    and GPT) or LSTMs (in earlier models like ELMo).
    These models are trained on massive corpora using language modeling objectives, such as:

- Causal Language Modeling (e.g., GPT): Predict the next token in a sequence.
- Masked Language Modeling (e.g., BERT): Predict randomly masked words in a sentence.
- Sequence-to-Sequence Modeling (e.g., T5, BART): Predict output sequences from input sequences. **Characteristics**
- Contextual: The same word can have different embeddings depending on its usage.
- Layered: Each layer of the model captures different linguistic features (e.g., syntax in early layers, semantics in
  deeper layers).
- Dynamic: Embeddings are generated on the fly for each new input, rather than being precomputed and fixed.
- Transferable: Embeddings can be fine-tuned or used across various downstream tasks (e.g., classification, question
  answering, summarization).
  **Pretraining** is the initial phase of training a language model on a large, unlabeled corpus of text data using a
  general-purpose objective. The goal is to enable the model to learn fundamental properties of language, such as
  syntax, semantics, and world knowledge, which can then be fine-tuned for specific downstream tasks (like sentiment
  analysis, question answering, or translation).
  Pretraining is **expensive**, and **self-supervised**. Once fine-tuned, you can reuse this model for multiple tasks
  within your domain. The fine-tuning step is generally much less expensive compared to the pretraining step.
  Essentially the same as transfer learning but we never created the inference part of the model to begin with.

15. **Word Embedding in Non-Textual Context**
    Word embeddings are typically used to represent words in a dense vector space. But the idea of embeddings‚Äîturning
    symbolic or categorical data into dense vectors that capture similarity‚Äîhas been applied far beyond just "words."
    1. **Item2Vec** is used to generate dense vector representations of items (e.g., songs, movies, books) based on user
       interactions ‚Äî typically used in recommendation systems.
       Inspired by: Word2Vec Skip-Gram
       How it works: Items that appear together in user sessions (like shopping carts, playlists, or browsing history)
       are treated like words in a sentence. The model learns to predict neighboring items given a current item ‚Äî much
       like Word2Vec predicts surrounding words.
       The result is that items that often co-occur in user behavior will have similar embeddings
    2. **Prod2Vec** (Product2Vec) extends Item2Vec with richer context ‚Äî often incorporating product metadata like
       categories, brands, or prices ‚Äî for even more meaningful embeddings.
    3. **Wav2Vec** is a deep learning model for learning representations of raw audio waveforms, used primarily in
       automatic speech recognition (0041SR). Developed by: Facebook AI (now Meta AI) How it works: Wav2Vec processes
       raw audio (e.g., .wav files) and learns a contextualized embedding of speech frames. It uses self-supervised
       learning: The model learns to predict masked segments of the audio signal using surrounding context (like BERT
       for audio). Wav2Vec 2.0 adds a transformer-based architecture and shows state-of-the-art performance on many ASR
       tasks.
16.  
