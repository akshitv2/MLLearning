{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "bdc80da6-6a50-48c3-d7a8-caa59e200417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2725865624711816 -1.3728111990668923\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(max([i for i in X_train[0]]),min([i for i in X_train[0]]))\n",
        "\n",
        "# 4. Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# 5. Define linear regression model\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(in_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1)  # Output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "model = LinearRegressionModel(X_train.shape[1])\n",
        "# 7. Training loop\n",
        "\n",
        "train_loss_to_plot = []\n",
        "test_loss_to_plot = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_Dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_Dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_Dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_Dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 6. Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 5000"
      ],
      "metadata": {
        "id": "uhz2eCHlM0km"
      },
      "id": "uhz2eCHlM0km",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5000\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (inputs, target) in enumerate(train_dataloader):\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        rmse_loss = torch.sqrt(loss)\n",
        "        train_loss_to_plot.append(rmse_loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        avg_loss = running_loss / len(train_dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "qjpnJBMXKJe7",
        "outputId": "7a08915d-fb16-4488-f2ff-b7b11cac016f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qjpnJBMXKJe7",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/5000], Loss: 0.2329\n",
            "Epoch [20/5000], Loss: 0.2305\n",
            "Epoch [30/5000], Loss: 0.2283\n",
            "Epoch [40/5000], Loss: 0.2262\n",
            "Epoch [50/5000], Loss: 0.2238\n",
            "Epoch [60/5000], Loss: 0.2207\n",
            "Epoch [70/5000], Loss: 0.2225\n",
            "Epoch [80/5000], Loss: 0.2177\n",
            "Epoch [90/5000], Loss: 0.2157\n",
            "Epoch [100/5000], Loss: 0.2160\n",
            "Epoch [110/5000], Loss: 0.2138\n",
            "Epoch [120/5000], Loss: 0.2114\n",
            "Epoch [130/5000], Loss: 0.2106\n",
            "Epoch [140/5000], Loss: 0.2090\n",
            "Epoch [150/5000], Loss: 0.2072\n",
            "Epoch [160/5000], Loss: 0.2056\n",
            "Epoch [170/5000], Loss: 0.2052\n",
            "Epoch [180/5000], Loss: 0.2041\n",
            "Epoch [190/5000], Loss: 0.2026\n",
            "Epoch [200/5000], Loss: 0.2022\n",
            "Epoch [210/5000], Loss: 0.2013\n",
            "Epoch [220/5000], Loss: 0.1992\n",
            "Epoch [230/5000], Loss: 0.1995\n",
            "Epoch [240/5000], Loss: 0.1978\n",
            "Epoch [250/5000], Loss: 0.1985\n",
            "Epoch [260/5000], Loss: 0.1960\n",
            "Epoch [270/5000], Loss: 0.1952\n",
            "Epoch [280/5000], Loss: 0.1936\n",
            "Epoch [290/5000], Loss: 0.1936\n",
            "Epoch [300/5000], Loss: 0.1920\n",
            "Epoch [310/5000], Loss: 0.1920\n",
            "Epoch [320/5000], Loss: 0.1919\n",
            "Epoch [330/5000], Loss: 0.1902\n",
            "Epoch [340/5000], Loss: 0.1887\n",
            "Epoch [350/5000], Loss: 0.1885\n",
            "Epoch [360/5000], Loss: 0.1871\n",
            "Epoch [370/5000], Loss: 0.1873\n",
            "Epoch [380/5000], Loss: 0.1880\n",
            "Epoch [390/5000], Loss: 0.1860\n",
            "Epoch [400/5000], Loss: 0.1845\n",
            "Epoch [410/5000], Loss: 0.1832\n",
            "Epoch [420/5000], Loss: 0.1826\n",
            "Epoch [430/5000], Loss: 0.1816\n",
            "Epoch [440/5000], Loss: 0.1821\n",
            "Epoch [450/5000], Loss: 0.1826\n",
            "Epoch [460/5000], Loss: 0.1805\n",
            "Epoch [470/5000], Loss: 0.1792\n",
            "Epoch [480/5000], Loss: 0.1803\n",
            "Epoch [490/5000], Loss: 0.1779\n",
            "Epoch [500/5000], Loss: 0.1794\n",
            "Epoch [510/5000], Loss: 0.1781\n",
            "Epoch [520/5000], Loss: 0.1769\n",
            "Epoch [530/5000], Loss: 0.1775\n",
            "Epoch [540/5000], Loss: 0.1756\n",
            "Epoch [550/5000], Loss: 0.1731\n",
            "Epoch [560/5000], Loss: 0.1745\n",
            "Epoch [570/5000], Loss: 0.1731\n",
            "Epoch [580/5000], Loss: 0.1740\n",
            "Epoch [590/5000], Loss: 0.1730\n",
            "Epoch [600/5000], Loss: 0.1729\n",
            "Epoch [610/5000], Loss: 0.1724\n",
            "Epoch [620/5000], Loss: 0.1722\n",
            "Epoch [630/5000], Loss: 0.1696\n",
            "Epoch [640/5000], Loss: 0.1699\n",
            "Epoch [650/5000], Loss: 0.1696\n",
            "Epoch [660/5000], Loss: 0.1695\n",
            "Epoch [670/5000], Loss: 0.1680\n",
            "Epoch [680/5000], Loss: 0.1693\n",
            "Epoch [690/5000], Loss: 0.1681\n",
            "Epoch [700/5000], Loss: 0.1677\n",
            "Epoch [710/5000], Loss: 0.1671\n",
            "Epoch [720/5000], Loss: 0.1662\n",
            "Epoch [730/5000], Loss: 0.1654\n",
            "Epoch [740/5000], Loss: 0.1651\n",
            "Epoch [750/5000], Loss: 0.1660\n",
            "Epoch [760/5000], Loss: 0.1648\n",
            "Epoch [770/5000], Loss: 0.1646\n",
            "Epoch [780/5000], Loss: 0.1642\n",
            "Epoch [790/5000], Loss: 0.1641\n",
            "Epoch [800/5000], Loss: 0.1636\n",
            "Epoch [810/5000], Loss: 0.1636\n",
            "Epoch [820/5000], Loss: 0.1639\n",
            "Epoch [830/5000], Loss: 0.1621\n",
            "Epoch [840/5000], Loss: 0.1616\n",
            "Epoch [850/5000], Loss: 0.1606\n",
            "Epoch [860/5000], Loss: 0.1612\n",
            "Epoch [870/5000], Loss: 0.1596\n",
            "Epoch [880/5000], Loss: 0.1599\n",
            "Epoch [890/5000], Loss: 0.1604\n",
            "Epoch [900/5000], Loss: 0.1592\n",
            "Epoch [910/5000], Loss: 0.1593\n",
            "Epoch [920/5000], Loss: 0.1598\n",
            "Epoch [930/5000], Loss: 0.1594\n",
            "Epoch [940/5000], Loss: 0.1586\n",
            "Epoch [950/5000], Loss: 0.1584\n",
            "Epoch [960/5000], Loss: 0.1578\n",
            "Epoch [970/5000], Loss: 0.1568\n",
            "Epoch [980/5000], Loss: 0.1574\n",
            "Epoch [990/5000], Loss: 0.1588\n",
            "Epoch [1000/5000], Loss: 0.1570\n",
            "Epoch [1010/5000], Loss: 0.1566\n",
            "Epoch [1020/5000], Loss: 0.1575\n",
            "Epoch [1030/5000], Loss: 0.1564\n",
            "Epoch [1040/5000], Loss: 0.1551\n",
            "Epoch [1050/5000], Loss: 0.1560\n",
            "Epoch [1060/5000], Loss: 0.1559\n",
            "Epoch [1070/5000], Loss: 0.1564\n",
            "Epoch [1080/5000], Loss: 0.1550\n",
            "Epoch [1090/5000], Loss: 0.1667\n",
            "Epoch [1100/5000], Loss: 0.1607\n",
            "Epoch [1110/5000], Loss: 0.1610\n",
            "Epoch [1120/5000], Loss: 0.1600\n",
            "Epoch [1130/5000], Loss: 0.1646\n",
            "Epoch [1140/5000], Loss: 0.1634\n",
            "Epoch [1150/5000], Loss: 0.1618\n",
            "Epoch [1160/5000], Loss: 0.1615\n",
            "Epoch [1170/5000], Loss: 0.1621\n",
            "Epoch [1180/5000], Loss: 0.1603\n",
            "Epoch [1190/5000], Loss: 0.1597\n",
            "Epoch [1200/5000], Loss: 0.1576\n",
            "Epoch [1210/5000], Loss: 0.1589\n",
            "Epoch [1220/5000], Loss: 0.1583\n",
            "Epoch [1230/5000], Loss: 0.1577\n",
            "Epoch [1240/5000], Loss: 0.1580\n",
            "Epoch [1250/5000], Loss: 0.1589\n",
            "Epoch [1260/5000], Loss: 0.1571\n",
            "Epoch [1270/5000], Loss: 0.1566\n",
            "Epoch [1280/5000], Loss: 0.1570\n",
            "Epoch [1290/5000], Loss: 0.1561\n",
            "Epoch [1300/5000], Loss: 0.1558\n",
            "Epoch [1310/5000], Loss: 0.1564\n",
            "Epoch [1320/5000], Loss: 0.1549\n",
            "Epoch [1330/5000], Loss: 0.1555\n",
            "Epoch [1340/5000], Loss: 0.1555\n",
            "Epoch [1350/5000], Loss: 0.1555\n",
            "Epoch [1360/5000], Loss: 0.1554\n",
            "Epoch [1370/5000], Loss: 0.1545\n",
            "Epoch [1380/5000], Loss: 0.1537\n",
            "Epoch [1390/5000], Loss: 0.1541\n",
            "Epoch [1400/5000], Loss: 0.1539\n",
            "Epoch [1410/5000], Loss: 0.1528\n",
            "Epoch [1420/5000], Loss: 0.1531\n",
            "Epoch [1430/5000], Loss: 0.1542\n",
            "Epoch [1440/5000], Loss: 0.1556\n",
            "Epoch [1450/5000], Loss: 0.1640\n",
            "Epoch [1460/5000], Loss: 0.1551\n",
            "Epoch [1470/5000], Loss: 0.1529\n",
            "Epoch [1480/5000], Loss: 0.1538\n",
            "Epoch [1490/5000], Loss: 0.1530\n",
            "Epoch [1500/5000], Loss: 0.1518\n",
            "Epoch [1510/5000], Loss: 0.1510\n",
            "Epoch [1520/5000], Loss: 0.1523\n",
            "Epoch [1530/5000], Loss: 0.1503\n",
            "Epoch [1540/5000], Loss: 0.1513\n",
            "Epoch [1550/5000], Loss: 0.1514\n",
            "Epoch [1560/5000], Loss: 0.1510\n",
            "Epoch [1570/5000], Loss: 0.1509\n",
            "Epoch [1580/5000], Loss: 0.1506\n",
            "Epoch [1590/5000], Loss: 0.1499\n",
            "Epoch [1600/5000], Loss: 0.1502\n",
            "Epoch [1610/5000], Loss: 0.1499\n",
            "Epoch [1620/5000], Loss: 0.1505\n",
            "Epoch [1630/5000], Loss: 0.1502\n",
            "Epoch [1640/5000], Loss: 0.1493\n",
            "Epoch [1650/5000], Loss: 0.1493\n",
            "Epoch [1660/5000], Loss: 0.1492\n",
            "Epoch [1670/5000], Loss: 0.1485\n",
            "Epoch [1680/5000], Loss: 0.1494\n",
            "Epoch [1690/5000], Loss: 0.1491\n",
            "Epoch [1700/5000], Loss: 0.1478\n",
            "Epoch [1710/5000], Loss: 0.1485\n",
            "Epoch [1720/5000], Loss: 0.1485\n",
            "Epoch [1730/5000], Loss: 0.1469\n",
            "Epoch [1740/5000], Loss: 0.1487\n",
            "Epoch [1750/5000], Loss: 0.1473\n",
            "Epoch [1760/5000], Loss: 0.1466\n",
            "Epoch [1770/5000], Loss: 0.1480\n",
            "Epoch [1780/5000], Loss: 0.1477\n",
            "Epoch [1790/5000], Loss: 0.1471\n",
            "Epoch [1800/5000], Loss: 0.1467\n",
            "Epoch [1810/5000], Loss: 0.1458\n",
            "Epoch [1820/5000], Loss: 0.1473\n",
            "Epoch [1830/5000], Loss: 0.1463\n",
            "Epoch [1840/5000], Loss: 0.1467\n",
            "Epoch [1850/5000], Loss: 0.1473\n",
            "Epoch [1860/5000], Loss: 0.1458\n",
            "Epoch [1870/5000], Loss: 0.1461\n",
            "Epoch [1880/5000], Loss: 0.1460\n",
            "Epoch [1890/5000], Loss: 0.1454\n",
            "Epoch [1900/5000], Loss: 0.1451\n",
            "Epoch [1910/5000], Loss: 0.1466\n",
            "Epoch [1920/5000], Loss: 0.1476\n",
            "Epoch [1930/5000], Loss: 0.1471\n",
            "Epoch [1940/5000], Loss: 0.1475\n",
            "Epoch [1950/5000], Loss: 0.1459\n",
            "Epoch [1960/5000], Loss: 0.1453\n",
            "Epoch [1970/5000], Loss: 0.1468\n",
            "Epoch [1980/5000], Loss: 0.1464\n",
            "Epoch [1990/5000], Loss: 0.1450\n",
            "Epoch [2000/5000], Loss: 0.1454\n",
            "Epoch [2010/5000], Loss: 0.1466\n",
            "Epoch [2020/5000], Loss: 0.1468\n",
            "Epoch [2030/5000], Loss: 0.1449\n",
            "Epoch [2040/5000], Loss: 0.1436\n",
            "Epoch [2050/5000], Loss: 0.1449\n",
            "Epoch [2060/5000], Loss: 0.1443\n",
            "Epoch [2070/5000], Loss: 0.1439\n",
            "Epoch [2080/5000], Loss: 0.1439\n",
            "Epoch [2090/5000], Loss: 0.1444\n",
            "Epoch [2100/5000], Loss: 0.1434\n",
            "Epoch [2110/5000], Loss: 0.1435\n",
            "Epoch [2120/5000], Loss: 0.1437\n",
            "Epoch [2130/5000], Loss: 0.1437\n",
            "Epoch [2140/5000], Loss: 0.1430\n",
            "Epoch [2150/5000], Loss: 0.1438\n",
            "Epoch [2160/5000], Loss: 0.1425\n",
            "Epoch [2170/5000], Loss: 0.1424\n",
            "Epoch [2180/5000], Loss: 0.1427\n",
            "Epoch [2190/5000], Loss: 0.1426\n",
            "Epoch [2200/5000], Loss: 0.1430\n",
            "Epoch [2210/5000], Loss: 0.1429\n",
            "Epoch [2220/5000], Loss: 0.1421\n",
            "Epoch [2230/5000], Loss: 0.1419\n",
            "Epoch [2240/5000], Loss: 0.1431\n",
            "Epoch [2250/5000], Loss: 0.1415\n",
            "Epoch [2260/5000], Loss: 0.1422\n",
            "Epoch [2270/5000], Loss: 0.1421\n",
            "Epoch [2280/5000], Loss: 0.1419\n",
            "Epoch [2290/5000], Loss: 0.1419\n",
            "Epoch [2300/5000], Loss: 0.1405\n",
            "Epoch [2310/5000], Loss: 0.1412\n",
            "Epoch [2320/5000], Loss: 0.1408\n",
            "Epoch [2330/5000], Loss: 0.1426\n",
            "Epoch [2340/5000], Loss: 0.1420\n",
            "Epoch [2350/5000], Loss: 0.1402\n",
            "Epoch [2360/5000], Loss: 0.1414\n",
            "Epoch [2370/5000], Loss: 0.1403\n",
            "Epoch [2380/5000], Loss: 0.1407\n",
            "Epoch [2390/5000], Loss: 0.1405\n",
            "Epoch [2400/5000], Loss: 0.1409\n",
            "Epoch [2410/5000], Loss: 0.1402\n",
            "Epoch [2420/5000], Loss: 0.1402\n",
            "Epoch [2430/5000], Loss: 0.1411\n",
            "Epoch [2440/5000], Loss: 0.1416\n",
            "Epoch [2450/5000], Loss: 0.1400\n",
            "Epoch [2460/5000], Loss: 0.1394\n",
            "Epoch [2470/5000], Loss: 0.1396\n",
            "Epoch [2480/5000], Loss: 0.1397\n",
            "Epoch [2490/5000], Loss: 0.1405\n",
            "Epoch [2500/5000], Loss: 0.1400\n",
            "Epoch [2510/5000], Loss: 0.1398\n",
            "Epoch [2520/5000], Loss: 0.1405\n",
            "Epoch [2530/5000], Loss: 0.1400\n",
            "Epoch [2540/5000], Loss: 0.1394\n",
            "Epoch [2550/5000], Loss: 0.1388\n",
            "Epoch [2560/5000], Loss: 0.1398\n",
            "Epoch [2570/5000], Loss: 0.1381\n",
            "Epoch [2580/5000], Loss: 0.1380\n",
            "Epoch [2590/5000], Loss: 0.1383\n",
            "Epoch [2600/5000], Loss: 0.1384\n",
            "Epoch [2610/5000], Loss: 0.1384\n",
            "Epoch [2620/5000], Loss: 0.1386\n",
            "Epoch [2630/5000], Loss: 0.1377\n",
            "Epoch [2640/5000], Loss: 0.1387\n",
            "Epoch [2650/5000], Loss: 0.1386\n",
            "Epoch [2660/5000], Loss: 0.1388\n",
            "Epoch [2670/5000], Loss: 0.1383\n",
            "Epoch [2680/5000], Loss: 0.1385\n",
            "Epoch [2690/5000], Loss: 0.1375\n",
            "Epoch [2700/5000], Loss: 0.1376\n",
            "Epoch [2710/5000], Loss: 0.1382\n",
            "Epoch [2720/5000], Loss: 0.1372\n",
            "Epoch [2730/5000], Loss: 0.1380\n",
            "Epoch [2740/5000], Loss: 0.1378\n",
            "Epoch [2750/5000], Loss: 0.1378\n",
            "Epoch [2760/5000], Loss: 0.1368\n",
            "Epoch [2770/5000], Loss: 0.1376\n",
            "Epoch [2780/5000], Loss: 0.1371\n",
            "Epoch [2790/5000], Loss: 0.1370\n",
            "Epoch [2800/5000], Loss: 0.1363\n",
            "Epoch [2810/5000], Loss: 0.1367\n",
            "Epoch [2820/5000], Loss: 0.1362\n",
            "Epoch [2830/5000], Loss: 0.1368\n",
            "Epoch [2840/5000], Loss: 0.1375\n",
            "Epoch [2850/5000], Loss: 0.1359\n",
            "Epoch [2860/5000], Loss: 0.1369\n",
            "Epoch [2870/5000], Loss: 0.1363\n",
            "Epoch [2880/5000], Loss: 0.1363\n",
            "Epoch [2890/5000], Loss: 0.1366\n",
            "Epoch [2900/5000], Loss: 0.1369\n",
            "Epoch [2910/5000], Loss: 0.1367\n",
            "Epoch [2920/5000], Loss: 0.1359\n",
            "Epoch [2930/5000], Loss: 0.1364\n",
            "Epoch [2940/5000], Loss: 0.1359\n",
            "Epoch [2950/5000], Loss: 0.1363\n",
            "Epoch [2960/5000], Loss: 0.1355\n",
            "Epoch [2970/5000], Loss: 0.1377\n",
            "Epoch [2980/5000], Loss: 0.1370\n",
            "Epoch [2990/5000], Loss: 0.1357\n",
            "Epoch [3000/5000], Loss: 0.1355\n",
            "Epoch [3010/5000], Loss: 0.1359\n",
            "Epoch [3020/5000], Loss: 0.1371\n",
            "Epoch [3030/5000], Loss: 0.1358\n",
            "Epoch [3040/5000], Loss: 0.1358\n",
            "Epoch [3050/5000], Loss: 0.1364\n",
            "Epoch [3060/5000], Loss: 0.1359\n",
            "Epoch [3070/5000], Loss: 0.1363\n",
            "Epoch [3080/5000], Loss: 0.1364\n",
            "Epoch [3090/5000], Loss: 0.1348\n",
            "Epoch [3100/5000], Loss: 0.1349\n",
            "Epoch [3110/5000], Loss: 0.1354\n",
            "Epoch [3120/5000], Loss: 0.1356\n",
            "Epoch [3130/5000], Loss: 0.1348\n",
            "Epoch [3140/5000], Loss: 0.1355\n",
            "Epoch [3150/5000], Loss: 0.1349\n",
            "Epoch [3160/5000], Loss: 0.1347\n",
            "Epoch [3170/5000], Loss: 0.1352\n",
            "Epoch [3180/5000], Loss: 0.1351\n",
            "Epoch [3190/5000], Loss: 0.1351\n",
            "Epoch [3200/5000], Loss: 0.1349\n",
            "Epoch [3210/5000], Loss: 0.1350\n",
            "Epoch [3220/5000], Loss: 0.1347\n",
            "Epoch [3230/5000], Loss: 0.1353\n",
            "Epoch [3240/5000], Loss: 0.1345\n",
            "Epoch [3250/5000], Loss: 0.1355\n",
            "Epoch [3260/5000], Loss: 0.1354\n",
            "Epoch [3270/5000], Loss: 0.1341\n",
            "Epoch [3280/5000], Loss: 0.1351\n",
            "Epoch [3290/5000], Loss: 0.1335\n",
            "Epoch [3300/5000], Loss: 0.1347\n",
            "Epoch [3310/5000], Loss: 0.1342\n",
            "Epoch [3320/5000], Loss: 0.1341\n",
            "Epoch [3330/5000], Loss: 0.1348\n",
            "Epoch [3340/5000], Loss: 0.1343\n",
            "Epoch [3350/5000], Loss: 0.1349\n",
            "Epoch [3360/5000], Loss: 0.1340\n",
            "Epoch [3370/5000], Loss: 0.1344\n",
            "Epoch [3380/5000], Loss: 0.1356\n",
            "Epoch [3390/5000], Loss: 0.1347\n",
            "Epoch [3400/5000], Loss: 0.1346\n",
            "Epoch [3410/5000], Loss: 0.1339\n",
            "Epoch [3420/5000], Loss: 0.1340\n",
            "Epoch [3430/5000], Loss: 0.1344\n",
            "Epoch [3440/5000], Loss: 0.1337\n",
            "Epoch [3450/5000], Loss: 0.1336\n",
            "Epoch [3460/5000], Loss: 0.1331\n",
            "Epoch [3470/5000], Loss: 0.1338\n",
            "Epoch [3480/5000], Loss: 0.1348\n",
            "Epoch [3490/5000], Loss: 0.1330\n",
            "Epoch [3500/5000], Loss: 0.1330\n",
            "Epoch [3510/5000], Loss: 0.1322\n",
            "Epoch [3520/5000], Loss: 0.1329\n",
            "Epoch [3530/5000], Loss: 0.1326\n",
            "Epoch [3540/5000], Loss: 0.1327\n",
            "Epoch [3550/5000], Loss: 0.1330\n",
            "Epoch [3560/5000], Loss: 0.1334\n",
            "Epoch [3570/5000], Loss: 0.1329\n",
            "Epoch [3580/5000], Loss: 0.1324\n",
            "Epoch [3590/5000], Loss: 0.1335\n",
            "Epoch [3600/5000], Loss: 0.1340\n",
            "Epoch [3610/5000], Loss: 0.1333\n",
            "Epoch [3620/5000], Loss: 0.1331\n",
            "Epoch [3630/5000], Loss: 0.1340\n",
            "Epoch [3640/5000], Loss: 0.1321\n",
            "Epoch [3650/5000], Loss: 0.1329\n",
            "Epoch [3660/5000], Loss: 0.1335\n",
            "Epoch [3670/5000], Loss: 0.1317\n",
            "Epoch [3680/5000], Loss: 0.1323\n",
            "Epoch [3690/5000], Loss: 0.1319\n",
            "Epoch [3700/5000], Loss: 0.1328\n",
            "Epoch [3710/5000], Loss: 0.1340\n",
            "Epoch [3720/5000], Loss: 0.1326\n",
            "Epoch [3730/5000], Loss: 0.1317\n",
            "Epoch [3740/5000], Loss: 0.1325\n",
            "Epoch [3750/5000], Loss: 0.1325\n",
            "Epoch [3760/5000], Loss: 0.1322\n",
            "Epoch [3770/5000], Loss: 0.1321\n",
            "Epoch [3780/5000], Loss: 0.1326\n",
            "Epoch [3790/5000], Loss: 0.1325\n",
            "Epoch [3800/5000], Loss: 0.1323\n",
            "Epoch [3810/5000], Loss: 0.1319\n",
            "Epoch [3820/5000], Loss: 0.1324\n",
            "Epoch [3830/5000], Loss: 0.1316\n",
            "Epoch [3840/5000], Loss: 0.1313\n",
            "Epoch [3850/5000], Loss: 0.1317\n",
            "Epoch [3860/5000], Loss: 0.1313\n",
            "Epoch [3870/5000], Loss: 0.1313\n",
            "Epoch [3880/5000], Loss: 0.1317\n",
            "Epoch [3890/5000], Loss: 0.1312\n",
            "Epoch [3900/5000], Loss: 0.1310\n",
            "Epoch [3910/5000], Loss: 0.1319\n",
            "Epoch [3920/5000], Loss: 0.1327\n",
            "Epoch [3930/5000], Loss: 0.1311\n",
            "Epoch [3940/5000], Loss: 0.1313\n",
            "Epoch [3950/5000], Loss: 0.1302\n",
            "Epoch [3960/5000], Loss: 0.1313\n",
            "Epoch [3970/5000], Loss: 0.1316\n",
            "Epoch [3980/5000], Loss: 0.1313\n",
            "Epoch [3990/5000], Loss: 0.1304\n",
            "Epoch [4000/5000], Loss: 0.1297\n",
            "Epoch [4010/5000], Loss: 0.1304\n",
            "Epoch [4020/5000], Loss: 0.1300\n",
            "Epoch [4030/5000], Loss: 0.1307\n",
            "Epoch [4040/5000], Loss: 0.1311\n",
            "Epoch [4050/5000], Loss: 0.1300\n",
            "Epoch [4060/5000], Loss: 0.1311\n",
            "Epoch [4070/5000], Loss: 0.1306\n",
            "Epoch [4080/5000], Loss: 0.1306\n",
            "Epoch [4090/5000], Loss: 0.1297\n",
            "Epoch [4100/5000], Loss: 0.1306\n",
            "Epoch [4110/5000], Loss: 0.1305\n",
            "Epoch [4120/5000], Loss: 0.1302\n",
            "Epoch [4130/5000], Loss: 0.1303\n",
            "Epoch [4140/5000], Loss: 0.1308\n",
            "Epoch [4150/5000], Loss: 0.1308\n",
            "Epoch [4160/5000], Loss: 0.1298\n",
            "Epoch [4170/5000], Loss: 0.1311\n",
            "Epoch [4180/5000], Loss: 0.1302\n",
            "Epoch [4190/5000], Loss: 0.1299\n",
            "Epoch [4200/5000], Loss: 0.1290\n",
            "Epoch [4210/5000], Loss: 0.1298\n",
            "Epoch [4220/5000], Loss: 0.1298\n",
            "Epoch [4230/5000], Loss: 0.1305\n",
            "Epoch [4240/5000], Loss: 0.1292\n",
            "Epoch [4250/5000], Loss: 0.1287\n",
            "Epoch [4260/5000], Loss: 0.1297\n",
            "Epoch [4270/5000], Loss: 0.1284\n",
            "Epoch [4280/5000], Loss: 0.1290\n",
            "Epoch [4290/5000], Loss: 0.1297\n",
            "Epoch [4300/5000], Loss: 0.1288\n",
            "Epoch [4310/5000], Loss: 0.1294\n",
            "Epoch [4320/5000], Loss: 0.1291\n",
            "Epoch [4330/5000], Loss: 0.1297\n",
            "Epoch [4340/5000], Loss: 0.1298\n",
            "Epoch [4350/5000], Loss: 0.1288\n",
            "Epoch [4360/5000], Loss: 0.1298\n",
            "Epoch [4370/5000], Loss: 0.1294\n",
            "Epoch [4380/5000], Loss: 0.1301\n",
            "Epoch [4390/5000], Loss: 0.1287\n",
            "Epoch [4400/5000], Loss: 0.1289\n",
            "Epoch [4410/5000], Loss: 0.1283\n",
            "Epoch [4420/5000], Loss: 0.1283\n",
            "Epoch [4430/5000], Loss: 0.1288\n",
            "Epoch [4440/5000], Loss: 0.1279\n",
            "Epoch [4450/5000], Loss: 0.1292\n",
            "Epoch [4460/5000], Loss: 0.1289\n",
            "Epoch [4470/5000], Loss: 0.1281\n",
            "Epoch [4480/5000], Loss: 0.1288\n",
            "Epoch [4490/5000], Loss: 0.1290\n",
            "Epoch [4500/5000], Loss: 0.1286\n",
            "Epoch [4510/5000], Loss: 0.1284\n",
            "Epoch [4520/5000], Loss: 0.1285\n",
            "Epoch [4530/5000], Loss: 0.1284\n",
            "Epoch [4540/5000], Loss: 0.1288\n",
            "Epoch [4550/5000], Loss: 0.1284\n",
            "Epoch [4560/5000], Loss: 0.1294\n",
            "Epoch [4570/5000], Loss: 0.1294\n",
            "Epoch [4580/5000], Loss: 0.1287\n",
            "Epoch [4590/5000], Loss: 0.1272\n",
            "Epoch [4600/5000], Loss: 0.1292\n",
            "Epoch [4610/5000], Loss: 0.1285\n",
            "Epoch [4620/5000], Loss: 0.1281\n",
            "Epoch [4630/5000], Loss: 0.1277\n",
            "Epoch [4640/5000], Loss: 0.1280\n",
            "Epoch [4650/5000], Loss: 0.1290\n",
            "Epoch [4660/5000], Loss: 0.1283\n",
            "Epoch [4670/5000], Loss: 0.1273\n",
            "Epoch [4680/5000], Loss: 0.1276\n",
            "Epoch [4690/5000], Loss: 0.1290\n",
            "Epoch [4700/5000], Loss: 0.1283\n",
            "Epoch [4710/5000], Loss: 0.1275\n",
            "Epoch [4720/5000], Loss: 0.1291\n",
            "Epoch [4730/5000], Loss: 0.1278\n",
            "Epoch [4740/5000], Loss: 0.1266\n",
            "Epoch [4750/5000], Loss: 0.1274\n",
            "Epoch [4760/5000], Loss: 0.1279\n",
            "Epoch [4770/5000], Loss: 0.1279\n",
            "Epoch [4780/5000], Loss: 0.1276\n",
            "Epoch [4790/5000], Loss: 0.1290\n",
            "Epoch [4800/5000], Loss: 0.1279\n",
            "Epoch [4810/5000], Loss: 0.1276\n",
            "Epoch [4820/5000], Loss: 0.1278\n",
            "Epoch [4830/5000], Loss: 0.1294\n",
            "Epoch [4840/5000], Loss: 0.1276\n",
            "Epoch [4850/5000], Loss: 0.1281\n",
            "Epoch [4860/5000], Loss: 0.1272\n",
            "Epoch [4870/5000], Loss: 0.1269\n",
            "Epoch [4880/5000], Loss: 0.1284\n",
            "Epoch [4890/5000], Loss: 0.1267\n",
            "Epoch [4900/5000], Loss: 0.1278\n",
            "Epoch [4910/5000], Loss: 0.1274\n",
            "Epoch [4920/5000], Loss: 0.1273\n",
            "Epoch [4930/5000], Loss: 0.1277\n",
            "Epoch [4940/5000], Loss: 0.1275\n",
            "Epoch [4950/5000], Loss: 0.1277\n",
            "Epoch [4960/5000], Loss: 0.1266\n",
            "Epoch [4970/5000], Loss: 0.1272\n",
            "Epoch [4980/5000], Loss: 0.1272\n",
            "Epoch [4990/5000], Loss: 0.1274\n",
            "Epoch [5000/5000], Loss: 0.1270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    # rmse_loss = torch.sqrt(loss)\n",
        "    # train_loss_to_plot.append(rmse_loss.item())\n",
        "\n",
        "    # y_pred = model(X_test_tensor)\n",
        "    # test_loss = criterion(y_pred, y_test_tensor)\n",
        "    # test_loss_to_plot.append(torch.sqrt(test_loss).item())\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {rmse_loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test_tensor)\n",
        "    test_loss = criterion(y_pred, y_test_tensor)\n",
        "    epsilon = 1e-7\n",
        "    # mape = torch.mean(torch.abs((y_test - y_pred) / (y_test + epsilon))) * 100\n",
        "    # print(f\"MAPE: {mape.item():.2f}%\")\n",
        "    print(f\"\\nTest MSE: {test_loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "zj9BtP6LKDz7",
        "outputId": "c392d6d4-7c03-424f-dcf4-49d05f92cb7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zj9BtP6LKDz7",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/5000], Loss: 0.6605\n",
            "Epoch [200/5000], Loss: 0.6324\n",
            "Epoch [300/5000], Loss: 0.6132\n",
            "Epoch [400/5000], Loss: 0.5987\n",
            "Epoch [500/5000], Loss: 0.5905\n",
            "Epoch [600/5000], Loss: 0.5825\n",
            "Epoch [700/5000], Loss: 0.5749\n",
            "Epoch [800/5000], Loss: 0.5691\n",
            "Epoch [900/5000], Loss: 0.5645\n",
            "Epoch [1000/5000], Loss: 0.5593\n",
            "Epoch [1100/5000], Loss: 0.5557\n",
            "Epoch [1200/5000], Loss: 0.5517\n",
            "Epoch [1300/5000], Loss: 0.5491\n",
            "Epoch [1400/5000], Loss: 0.5456\n",
            "Epoch [1500/5000], Loss: 0.5413\n",
            "Epoch [1600/5000], Loss: 0.5402\n",
            "Epoch [1700/5000], Loss: 0.5385\n",
            "Epoch [1800/5000], Loss: 0.5354\n",
            "Epoch [1900/5000], Loss: 0.5342\n",
            "Epoch [2000/5000], Loss: 0.5305\n",
            "Epoch [2100/5000], Loss: 0.5294\n",
            "Epoch [2200/5000], Loss: 0.5287\n",
            "Epoch [2300/5000], Loss: 0.5252\n",
            "Epoch [2400/5000], Loss: 0.5241\n",
            "Epoch [2500/5000], Loss: 0.5230\n",
            "Epoch [2600/5000], Loss: 0.5232\n",
            "Epoch [2700/5000], Loss: 0.5225\n",
            "Epoch [2800/5000], Loss: 0.5190\n",
            "Epoch [2900/5000], Loss: 0.5169\n",
            "Epoch [3000/5000], Loss: 0.5182\n",
            "Epoch [3100/5000], Loss: 0.5173\n",
            "Epoch [3200/5000], Loss: 0.5152\n",
            "Epoch [3300/5000], Loss: 0.5144\n",
            "Epoch [3400/5000], Loss: 0.5124\n",
            "Epoch [3500/5000], Loss: 0.5104\n",
            "Epoch [3600/5000], Loss: 0.5099\n",
            "Epoch [3700/5000], Loss: 0.5095\n",
            "Epoch [3800/5000], Loss: 0.5094\n",
            "Epoch [3900/5000], Loss: 0.5082\n",
            "Epoch [4000/5000], Loss: 0.5062\n",
            "Epoch [4100/5000], Loss: 0.5050\n",
            "Epoch [4200/5000], Loss: 0.5029\n",
            "Epoch [4300/5000], Loss: 0.5025\n",
            "Epoch [4400/5000], Loss: 0.5026\n",
            "Epoch [4500/5000], Loss: 0.5007\n",
            "Epoch [4600/5000], Loss: 0.5009\n",
            "Epoch [4700/5000], Loss: 0.4998\n",
            "Epoch [4800/5000], Loss: 0.5037\n",
            "Epoch [4900/5000], Loss: 0.4977\n",
            "Epoch [5000/5000], Loss: 0.5005\n",
            "\n",
            "Test MSE: 0.2835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "i7hp49vor3Xu"
      },
      "id": "i7hp49vor3Xu",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(test_loss_to_plot)\n",
        "plt.plot(train_loss_to_plot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "qxhYc0DTr4YD",
        "outputId": "f1212226-f3b0-4985-d78a-906acf7b9e46"
      },
      "id": "qxhYc0DTr4YD",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7acf77e31890>]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOB-5AnDvZT9",
        "outputId": "5ca19559-23c6-477b-b0fd-414e09a70f8d"
      },
      "id": "jOB-5AnDvZT9",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(4.526)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y_Rw2Qarnsa",
        "outputId": "2942f81d-9865-414c-e284-19eca449ad12"
      },
      "id": "5y_Rw2Qarnsa",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.8930\n",
            "Epoch [200/1000], Loss: 0.8016\n",
            "Epoch [300/1000], Loss: 0.7659\n",
            "Epoch [400/1000], Loss: 0.7363\n",
            "Epoch [500/1000], Loss: 0.7118\n",
            "Epoch [600/1000], Loss: 0.6930\n",
            "Epoch [700/1000], Loss: 0.6793\n",
            "Epoch [800/1000], Loss: 0.6694\n",
            "Epoch [900/1000], Loss: 0.6617\n",
            "Epoch [1000/1000], Loss: 0.6553\n",
            "\n",
            "Test MSE: 0.4513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sqrt(test_loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "Zg_Duj0TvigC",
        "outputId": "65457596-46a9-4ab8-af3c-2bcfa968af8e"
      },
      "id": "Zg_Duj0TvigC",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sqrt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-958829118.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sqrt' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}