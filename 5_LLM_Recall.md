# Questions for Recall from 5_LLM.md

## Foundations and basics
- What does 'n' represent in n-grams, and provide an example using the sentence "Hi I am Sam"? [#n-grams](5_LLM.md#n-grams)
- How are 2-grams and 3-grams constructed from a sentence? [#n-grams](5_LLM.md#n-grams)
- What is the Bag of Words (BoW) model, and how does it convert the sentence "The cat sat on the mat" into a vector using a given vocabulary? [#bag-of-words](5_LLM.md#bag-of-words)
- Why doesn't the Bag of Words model consider word order? [#bag-of-words](5_LLM.md#bag-of-words)
- What is TF-IDF, and how is it calculated as the product of Term Frequency (TF) and Inverse Document Frequency (IDF)? [#tf-idf](5_LLM.md#tf-idf)
- Explain the idea behind TF-IDF: how does it address biases in common words and highlight rare, significant ones? [#tf-idf](5_LLM.md#tf-idf)
- What is Byte Pair Encoding (BPE) in modern tokenization, and describe the iterative merging process with an example like "h e l l o"? [#byte-pair-encoding](5_LLM.md#byte-pair-encoding)
- How does Word Piece Encoding differ from Byte Pair Encoding, and what mathematical formula does it use to maximize log likelihood? [#word-piece-encoding](5_LLM.md#word-piece-encoding)
- What is the purpose of Positional Encoding, and how is it added to embeddings? [#usage](5_LLM.md#usage)
- Describe the Sinusoidal Positional Encoding formulas for even and odd dimensions, including the variables p, i, and d_model. [#sinusoidal](5_LLM.md#sinusoidal)
- What are Learned Positional Encodings, and what potential issue do they have compared to sinusoidal ones? [#learned](5_LLM.md#learned)
- Why do positional encodings make the same word at different positions appear different to the model? [#positional-encoding](5_LLM.md#positional-encoding)
- What is Contrastive Learning, and how does it teach models to recognize similarities and differences, including through data augmentation? [#contrastive-learning](5_LLM.md#contrastive-learning)
- In LLMs, how are positive pairs created for Contrastive Learning using text augmentation techniques like synonym replacement or masking? [#types](5_LLM.md#types)
- What are Semantic Pairs and Instruction Response Pairs in Contrastive Learning for LLMs? [#types](5_LLM.md#types)
- Name three applications of Contrastive Learning in LLMs, such as pretraining for embeddings, RAG, and instruction fine-tuning. [#applications](5_LLM.md#applications)

## Large Language Models
- What architecture do Large Language Models (LLMs) primarily use, and what mechanisms are key to them? [#large-language-models](5_LLM.md#large-language-models)
- Describe the Pre-Training process for LLMs, including what they learn from massive text corpora. [#pre-training](5_LLM.md#pre-training)
- What is Fine-Tuning in LLMs, and why is it used? [#fine-tuning](5_LLM.md#fine-tuning)
- What are Multimodal Large Language Models (MLLMs), and what types of data do they integrate? [#multimodal-large-language-models-mllms](5_LLM.md#multimodal-large-language-models-mllms)
- Explain Agentic Systems in LLMs, including their use of tools, planning, and frameworks like ReAct. [#agentic-systems](5_LLM.md#agentic-systems)
- What are Advanced Reasoning Models optimized for, and how do they extend base models? [#advanced-reasoning-models](5_LLM.md#advanced-reasoning-models)
- What is Chain of Thought Prompting, and give an example related to math problems? [#chain-of-thought-prompting](5_LLM.md#chain-of-thought-prompting)
- How does Self Consistency improve reliability in reasoning models? [#self-consistency](5_LLM.md#self-consistency)
- What is Tool Use in advanced reasoning, and what external systems might it involve? [#tool-use](5_LLM.md#tool-use)
- Describe Tree/Graph Based Reasoning and how it differs from linear chains. [#treegraph-based-reasoning](5_LLM.md#treegraph-based-reasoning)
- How do RLHF and RLAIF validate reasoning in advanced models? [#using-rlhf-reinforcement-learning-from-human-feedback-rlaif-reinforcement-learning-from-ai-feedback](5_LLM.md#using-rlhf-reinforcement-learning-from-human-feedback-rlaif-reinforcement-learning-from-ai-feedback)
- What are Small Language Models (SMLs), and why are they suitable for resource-constrained environments? [#small-language-models-smls](5_LLM.md#small-language-models-smls)

## RAG : Retrieval Augmented Generation
- What is Retrieval Augmented Generation (RAG), and how does it combine retrieval with LLMs? [#rag--retrieval-augmented-generation](5_LLM.md#rag--retrieval-augmented-generation)
- What are the two main components of RAG? [#composed-of](5_LLM.md#composed-of)
- Describe the role of the Retriever in RAG. [#retriever](5_LLM.md#retriever)
- What does the Generator do in RAG? [#generator](5_LLM.md#generator)
- Outline the steps in the Indexing Phase of RAG's working process. [#indexing-phase-the-preparation](5_LLM.md#indexing-phase-the-preparation)
- Describe the Retrieval and Generation steps in RAG, including query embedding and prompt augmentation. [#retrieval-and-generation](5_LLM.md#retrieval-and-generation)
- What is Sparse Retrieval, and name a common example? [#sparse-retrieval](5_LLM.md#sparse-retrieval)
- How does BM25 work in sparse retrieval, and what are its pros and cons? [#bm25](5_LLM.md#bm25)
- Explain Dense Retrieval and its advantages over sparse methods in understanding semantics. [#dense-retrieval](5_LLM.md#dense-retrieval)
- What are Dual Encoders/Bi-encoders in dense retrieval? [#dual-encodersbi-encoders](5_LLM.md#dual-encodersbi-encoders)
- How is Similarity Search performed in dense retrieval? [#similarity-search](5_LLM.md#similarity-search)
- Name four notable implementations of dense retrieval, such as DPR and ColBERT. [#notable-implementations](5_LLM.md#notable-implementations)
- What is Dense Passage Retrieval (DPR), and how does it use negatives? [#dense-passage-retrieval-dpr](5_LLM.md#dense-passage-retrieval-dpr)
- Describe ColBERT and its token-level vector approach. [#colbert](5_LLM.md#colbert)
- What algorithms are used in dense retrieval, such as ANN-based ones? [#algorithms-used](5_LLM.md#algorithms-used)
- Explain Hierarchical Navigable Small World (HNSW) for ANN search. [#hierarchical-navigable-small-world-hnsw](5_LLM.md#hierarchical-navigable-small-world-hnsw)
- What is FAISS, and what is it used for? [#faiss-facebook-ai-similarity-search](5_LLM.md#faiss-facebook-ai-similarity-search)
- Describe ScaNN and its focus on large-scale datasets. [#scann-scalable-nearest-neighbors](5_LLM.md#scann-scalable-nearest-neighbors)
- What is Hybrid Search in RAG? [#hybrid-search](5_LLM.md#hybrid-search)
- Explain Recursive Retrieval and its iterative process. [#recursive-retrieval](5_LLM.md#recursive-retrieval)

## Prompt Engineering
- What is Prompt Engineering? [#prompt-engineering](5_LLM.md#prompt-engineering)
- Describe Zero-Shot Prompting and its basic form. [#zero-shot-prompting](5_LLM.md#zero-shot-prompting)
- What is Few-Shot Prompting, and provide an example with English-to-French translations? [#few-shot-prompting](5_LLM.md#few-shot-prompting)
- How does Chain-of-Thought (CoT) Prompting work? [#chain-of-thought-cot-prompting](5_LLM.md#chain-of-thought-cot-prompting)
- Explain Role-Playing in prompt techniques. [#role-playing](5_LLM.md#role-playing)
- What is Instruction Tuning in prompts? [#instruction-tuning](5_LLM.md#instruction-tuning)
- What is Catastrophic Forgetting, and when does it occur? [#catastrophic-forgetting](5_LLM.md#catastrophic-forgetting)
- Name three solutions to Catastrophic Forgetting, such as Replay and Elastic Weight Consolidation. [#catastrophic-forgetting-solutions](5_LLM.md#catastrophic-forgetting-solutions)
- How does Replay work as a solution to forgetting? [#replay](5_LLM.md#replay)
- Explain Elastic Weight Consolidation using the Fisher Information Matrix. [#elastic-weight-consolidation](5_LLM.md#elastic-weight-consolidation)
- What are Dynamic Architectures for preventing forgetting? [#dynamic-architectures](5_LLM.md#dynamic-architectures)
- What causes Hallucination in models, and why do they occur? [#hallucination](5_LLM.md#hallucination)
- Name five solutions to Hallucination, including RAG and Chain of Thought. [#hallucination-solution](5_LLM.md#hallucination-solution)
- What is Repetition in LLMs, and why does it happen? [#repetition](5_LLM.md#repetition)
- Describe Degeneration and possible solutions like temperature adjustment or RLHF. [#degeneration](5_LLM.md#degeneration)
- What are Adversarial Prompts, and name five solutions like Instruction Separation? [#adversarial-prompts](5_LLM.md#adversarial-prompts)
- Explain Context Rot and its cause in longer contexts. [#context-rot](5_LLM.md#context-rot)
- What solution is suggested for Context Rot? [#solution](5_LLM.md#solution)
- What is Perplexity, and provide its formula? [#perplexity](5_LLM.md#perplexity)
- Explain BLEU score, including its two parts: Brevity Penalty and the n-gram overlap score. [#bleu-bilingual-evaluation-understudy](5_LLM.md#bleu-bilingual-evaluation-understudy)
- What is the formula for Brevity Penalty in BLEU? [#bp-brevity-penalty](5_LLM.md#bp-brevity-penalty)
- How is the BLEU score calculated overall? [#score](5_LLM.md#score)
- What is ROUGE, and how does ROUGE-N work? [#rouge-recall-oriented-understudy-for-gisting-evaluation](5_LLM.md#rouge-recall-oriented-understudy-for-gisting-evaluation)
- Describe ROUGE-L and its use of longest common subsequence. [#rouge-l](5_LLM.md#rouge-l)
- What are ROUGE-W and ROUGE-S/SU? [#rouge-w](5_LLM.md#rouge-w)
- How are Recall, Precision, and F1 Score adapted for text evaluation using n-grams? [#recall-precision-f1score](5_LLM.md#recall-precision-f1score)
- What makes METEOR more lenient than other metrics, and how does it calculate scores? [#meteor-metric-for-evaluation-of-translation-with-explicit-ordering](5_LLM.md#meteor-metric-for-evaluation-of-translation-with-explicit-ordering)
- For code evaluation, what is Exact Match, and why is it overly strict? [#exact-match](5_LLM.md#exact-match)
- Explain Pass@k and provide a calculation example with 5 samples. [#passk](5_LLM.md#passk)
- What is TruthfulQA for evaluating truthfulness? [#truthfulqa](5_LLM.md#truthfulqa)
- Name benchmarks for general understanding, maths/reasoning, and fact-checking. [#general-mmlu-massive-multitask-language-understanding](5_LLM.md#general-mmlu-massive-multitask-language-understanding)
- Provide the formulas for Recall@k and Precision@k in RAG metrics. [#recallk](5_LLM.md#recallk)
- What is NDCG, and explain its formula? [#ndcg-normalized-discounted-cumulative-gain](5_LLM.md#ndcg-normalized-discounted-cumulative-gain)
- Describe MRR and its formula. [#mrr-mean-reciprocal-rank](5_LLM.md#mrr-mean-reciprocal-rank)
- What are the drawbacks of Full Fine-Tuning? [#full-fine-tuning](5_LLM.md#full-fine-tuning)
- Explain Adapters in Parameter Efficient Fine-Tuning. [#adapters](5_LLM.md#adapters)
- What is Prompt Tuning, and how does it use soft prompts? [#prompt-tuning](5_LLM.md#prompt-tuning)
- Describe LoRA and how it uses low-rank matrices A and B. [#lora-low-rank-adaption](5_LLM.md#lora-low-rank-adaption)
- What is Prefix Tuning? [#prefix-tuning](5_LLM.md#prefix-tuning)
- What are Vector DBS in RAG-specific contexts? [#vector-dbs](5_LLM.md#vector-dbs)
- What is Greedy Decoding in text generation? [#greedy-decoding](5_LLM.md#greedy-decoding)
- Explain Beam Search and how it explores top k probabilities. [#beam-search](5_LLM.md#beam-search)
- How does Top K Sampling work, and what effect does higher K have? [#top-k-sampling](5_LLM.md#top-k-sampling)
- Describe Top P Sampling and why it's useful over Top K. [#top-p-sampling](5_LLM.md#top-p-sampling)
- What is Temperature Scaling, and provide its formula? [#temperature-scaling](5_LLM.md#temperature-scaling)
- What is Human Centric Eval? [#human-centric-eval](5_LLM.md#human-centric-eval)