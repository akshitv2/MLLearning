{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eaab9a6-1c01-4808-a053-ff3b303b39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import sys\n",
    "import copy\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b864ee62-d775-46a2-af6b-0385ae59e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(zip_path, dest_dir, overwrite=False):\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise FileNotFoundError(f\"ZIP file not found: {zip_path}\")\n",
    "\n",
    "    # Ensure destination directory exists\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        if overwrite:\n",
    "            # Remove existing files in destination if they exist\n",
    "            for file in zip_ref.namelist():\n",
    "                file_path = os.path.join(dest_dir, file)\n",
    "                if os.path.exists(file_path):\n",
    "                    if os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)\n",
    "                    else:\n",
    "                        os.remove(file_path)\n",
    "        zip_ref.extractall(dest_dir)\n",
    "        print(f\"Extracted '{zip_path}' to '{dest_dir}'\")\n",
    "\n",
    "def extract_images_label_by_folder(zip_path, dest_dir, image_size=(224, 224),overwrite=False, zip_subdirectory = \"\",batch_size=32):\n",
    "    extract_zip(zip_path, dest_dir, overwrite=False)\n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "            dest_dir + \"/dataset\",\n",
    "            labels=\"inferred\",\n",
    "            label_mode=\"int\",\n",
    "            class_names=None,\n",
    "            color_mode='rgb',\n",
    "            image_size=(224, 224),\n",
    "            interpolation=\"nearest\",\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            subset=\"training\",\n",
    "            seed=123)\n",
    "\n",
    "    test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "            dest_dir + \"/dataset\",\n",
    "            labels=\"inferred\",\n",
    "            label_mode=\"int\",\n",
    "            class_names=None,\n",
    "            interpolation=\"nearest\",\n",
    "            color_mode='rgb',\n",
    "            image_size=(224, 224),\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            subset=\"validation\",\n",
    "            seed=123)\n",
    "    return train_dataset,test_dataset\n",
    "def get_class_distribution(dataset):\n",
    "    class_counts = {}\n",
    "    for label in range(len(dataset.class_names)):\n",
    "        class_counts[label] = 0\n",
    "    for image, label in dataset.unbatch():\n",
    "        class_counts[label.numpy()]+=1\n",
    "    return class_counts\n",
    "\n",
    "def plot_class_distribution(class_counts):\n",
    "    sorted_class_counts = dict(sorted(class_counts.items(), key=lambda item: class_counts[item[0]], reverse=True))\n",
    "    labels = list(sorted_class_counts.keys())\n",
    "    values = list(sorted_class_counts.values())\n",
    "    plt.figure(figsize=(15, 6))  # Adjust figure size for better readability\n",
    "    plt.bar(labels, values)\n",
    "    plt.xlabel(\"Class Labels\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility if needed\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "    plt.show()\n",
    "\n",
    "def balance_images_distribution(dataset, class_counts, plot_dist = False, top_p_to_avg = 0.1):\n",
    "\n",
    "    def get_avg_entry_count(top_p_to_avg = 0.1):\n",
    "        sorted_counts = dict(sorted(class_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "        values = list(sorted_counts.values())\n",
    "\n",
    "        top_percent_index = int(len(values) * top_p_to_avg)\n",
    "        top_percent_values = values[:top_percent_index]\n",
    "        average_top_p = np.mean(top_percent_values)\n",
    "        print(f\"Average number of values for top p% entries: {average_top_p}\")\n",
    "        return average_top_p\n",
    "\n",
    "    def augment(image_orig):\n",
    "        image = tf.image.random_flip_left_right(image_orig)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "        image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "        return image\n",
    "\n",
    "    # Function to add an entry\n",
    "    def add_entry(images, labels, image, label):\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    new_images = []\n",
    "    new_labels = []\n",
    "    avg_entry_count = get_avg_entry_count(top_p_to_avg)\n",
    "\n",
    "    while(get_avg_entry_count(0.9) + 1 < avg_entry_count):\n",
    "        for images, labels in dataset.unbatch():\n",
    "            if(class_counts[labels.numpy()] < avg_entry_count):\n",
    "                add_entry(new_images, new_labels,augment(images),labels)\n",
    "                class_counts[labels.numpy()] += 1\n",
    "    if(plot_dist):\n",
    "        plot_class_distribution(class_counts)\n",
    "    return tf.data.Dataset.from_tensor_slices((new_images, new_labels))\n",
    "\n",
    "def one_hot_encode(image, label):\n",
    "  return image, tf.one_hot(label, depth=len(classes))\n",
    "\n",
    "def filter_p_percent(class_dist, p = 0.2):\n",
    "    return (sorted(class_dist.keys(), key = lambda x: class_dist[x], reverse=True))[0:int(len(class_dist.keys())*p)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84a1cce-a154-44ea-8918-b6ccaa18ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 'G:\\Datasets\\Pokemon151to10k.zip' to 'G:\\Temp'\n",
      "Found 10658 files belonging to 149 classes.\n",
      "Using 8527 files for training.\n",
      "Found 10658 files belonging to 149 classes.\n",
      "Using 2131 files for validation.\n"
     ]
    }
   ],
   "source": [
    "zip_file_path = r\"G:\\Datasets\\Pokemon151to10k.zip\"\n",
    "destination_directory = r\"G:\\Temp\" #Replace with your temporary directory\n",
    "unencoded_train_dataset,unencoded_test_dataset = extract_images_label_by_folder(zip_file_path, destination_directory, overwrite=False,batch_size=32)\n",
    "classes = unencoded_train_dataset.class_names\n",
    "class_dist = get_class_distribution(unencoded_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001a65c6-52e5-4f25-b9ec-3aea9a808022",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = unencoded_train_dataset.map(one_hot_encode)\n",
    "test_dataset = unencoded_test_dataset.map(one_hot_encode)\n",
    "# del unencoded_train_dataset\n",
    "# del unencoded_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4edc600-c62f-4452-b2b2-33b982b48990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.uint8, name=None),\n",
       " TensorSpec(shape=(None, 149), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "163338d5-a29b-4b5e-be96-67428777b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained VGG16 model without the top classification layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3)) # Adjust input_shape if needed\n",
    "# Freeze all layers in the base model\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(4096)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dense(4096)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "predictions = Dense(149, activation='softmax')(x)\n",
    "\n",
    "# Create the new model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with a slow learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4) # Example: A very slow learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998c961-8976-4f3e-a52b-116beaae9e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "267/267 [==============================] - ETA: 0s - loss: 3.0070 - accuracy: 0.4060   "
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcd402-3892-4cfb-b4aa-28bf7eac05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[-4:]:  # Unfreezing last 4 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train again with some layers unfrozen\n",
    "model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
