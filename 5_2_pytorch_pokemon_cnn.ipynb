{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CyBXljylLHE",
        "outputId": "23063259-3333-4979-809f-2b2641e90a77"
      },
      "id": "5CyBXljylLHE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3a7e6d829c419bcc"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "!unzip /content/drive/MyDrive/Datasets/Pokemon151to10k.zip -d /content/Pokemon151to10k"
      ],
      "id": "3a7e6d829c419bcc"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "spAJZCMmo8d5"
      },
      "id": "spAJZCMmo8d5",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "# 1. Define transformations for preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # resize all images to 224x224\n",
        "    transforms.ToTensor(),          # convert images to PyTorch tensors\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],                          std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "SxwWW84woSOj"
      },
      "id": "SxwWW84woSOj",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root='/content/Pokemon151to10k/dataset', transform=transform)\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "test_size  = len(dataset) - train_size  # remaining 20%\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "Uj7yH0gnoZA9"
      },
      "id": "Uj7yH0gnoZA9",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classesSet = set()\n",
        "for data in dataset:\n",
        "  classesSet.add(data[1])\n",
        "num_classes = len(classesSet)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mClVMPq0qVVl",
        "outputId": "423734e6-7617-4f9a-d6ed-7099f0c9f963"
      },
      "id": "mClVMPq0qVVl",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PokemonRecognizer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PokemonRecognizer, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten_size = 64 * 28 * 28\n",
        "        self.fc = nn.Linear(self.flatten_size, 128)\n",
        "        self.deep_output = nn.Linear(128, 1)\n",
        "        self.class_output = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution + Pooling\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)  # batch_size x flatten_size\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = torch.relu(self.fc(x))\n",
        "\n",
        "        # Two heads\n",
        "        deep_out = self.deep_output(x)      # e.g., regression output\n",
        "        class_out = self.class_output(x)    # classification output\n",
        "\n",
        "        return deep_out, class_out"
      ],
      "metadata": {
        "id": "ej_5BgnOp70o"
      },
      "id": "ej_5BgnOp70o",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PokemonRecognizer()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "jJD6Fr_spDYD"
      },
      "id": "jJD6Fr_spDYD",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for inputs, target in train_loader:\n",
        "    print(inputs.shape)\n",
        "    print(target.shape)\n",
        "    print(model(inputs))\n",
        "    # print(np.argmax(model(inputs)))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1kj2oWk2nrF",
        "outputId": "513f65f8-7525-4bdc-f5fe-117eabe8a60e"
      },
      "id": "b1kj2oWk2nrF",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32])\n",
            "(tensor([[0.0325],\n",
            "        [0.0357],\n",
            "        [0.0399],\n",
            "        [0.0317],\n",
            "        [0.0361],\n",
            "        [0.0394],\n",
            "        [0.0320],\n",
            "        [0.0332],\n",
            "        [0.0396],\n",
            "        [0.0324],\n",
            "        [0.0342],\n",
            "        [0.0328],\n",
            "        [0.0345],\n",
            "        [0.0347],\n",
            "        [0.0393],\n",
            "        [0.0313],\n",
            "        [0.0374],\n",
            "        [0.0334],\n",
            "        [0.0322],\n",
            "        [0.0301],\n",
            "        [0.0335],\n",
            "        [0.0423],\n",
            "        [0.0409],\n",
            "        [0.0263],\n",
            "        [0.0351],\n",
            "        [0.0303],\n",
            "        [0.0367],\n",
            "        [0.0344],\n",
            "        [0.0352],\n",
            "        [0.0357],\n",
            "        [0.0303],\n",
            "        [0.0356]], grad_fn=<AddmmBackward0>), tensor([[-0.0091, -0.0523,  0.0426,  ...,  0.0582, -0.0179, -0.0549],\n",
            "        [-0.0047, -0.0443,  0.0387,  ...,  0.0647, -0.0030, -0.0574],\n",
            "        [-0.0091, -0.0456,  0.0406,  ...,  0.0624, -0.0097, -0.0566],\n",
            "        ...,\n",
            "        [ 0.0002, -0.0486,  0.0395,  ...,  0.0626, -0.0073, -0.0551],\n",
            "        [-0.0063, -0.0493,  0.0301,  ...,  0.0487, -0.0261, -0.0593],\n",
            "        [-0.0099, -0.0424,  0.0383,  ...,  0.0601, -0.0024, -0.0515]],\n",
            "       grad_fn=<AddmmBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "        outputs = model(inputs)\n",
        "        # Access the classification output (second element of the tuple)\n",
        "        classification_outputs = outputs[1]\n",
        "        loss = criterion(classification_outputs, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    if (epoch + 1) % 1 == 0: # Print loss every epoch\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSw_CXdk17JK",
        "outputId": "7de5d5e7-2314-4ab9-c461-2eaffbb6fbfa"
      },
      "id": "nSw_CXdk17JK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_and_visualize(model, test_loader, dataset, num_images=5):\n",
        "    model.eval()\n",
        "    class_names = dataset.classes # Get class names from the dataset\n",
        "\n",
        "    images_displayed = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, target in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs[1].data, 1)\n",
        "\n",
        "            for i in range(inputs.size(0)):\n",
        "                if images_displayed < num_images:\n",
        "                    img = inputs[i].cpu().numpy().transpose((1, 2, 0))\n",
        "                    actual_class = class_names[target[i]]\n",
        "                    predicted_class = class_names[predicted[i]]\n",
        "\n",
        "                    plt.imshow(img)\n",
        "                    plt.title(f\"Actual: {actual_class}, Predicted: {predicted_class}\")\n",
        "                    plt.axis('off')\n",
        "                    plt.show()\n",
        "\n",
        "                    images_displayed += 1\n",
        "                else:\n",
        "                    break\n",
        "            if images_displayed >= num_images:\n",
        "                break\n",
        "\n",
        "# Example usage after training:\n",
        "# evaluate_and_visualize(model, test_loader, dataset)"
      ],
      "metadata": {
        "id": "0383SPXk3oLD"
      },
      "id": "0383SPXk3oLD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}