{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-08-10T14:18:55.361917Z",
          "start_time": "2025-08-10T14:18:48.012989Z"
        },
        "id": "initial_id"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from sklearn import datasets\n",
        "import time\n",
        "\n",
        "#1.2 Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SimModeExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing Configs\n",
        "for k, v in spark.sparkContext.getConf().getAll():\n",
        "    print(f\"{k} = {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEmm_yFKcz0W",
        "outputId": "44ef34b1-6167-4b1b-d9c7-406079d90b6f"
      },
      "id": "DEmm_yFKcz0W",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark.driver.host = 308b902d5893\n",
            "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.driver.port = 40171\n",
            "spark.app.id = local-1755146982896\n",
            "spark.executor.id = driver\n",
            "spark.app.submitTime = 1755146980571\n",
            "spark.app.startTime = 1755146980902\n",
            "spark.rdd.compress = True\n",
            "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.serializer.objectStreamReset = 100\n",
            "spark.master = local[*]\n",
            "spark.submit.pyFiles = \n",
            "spark.submit.deployMode = client\n",
            "spark.app.name = SimModeExample\n",
            "spark.ui.showConsoleProgress = true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"Apache Spark is an open-source distributed general-purpose cluster-computing framework.\n",
        "It provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\n",
        "Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries, and streaming.\"\"\"\n",
        "\n",
        "with open(\"sample_text.txt\", \"w\") as f:\n",
        "    f.write(sample_text)"
      ],
      "metadata": {
        "id": "XRGAKBTaigSI"
      },
      "id": "XRGAKBTaigSI",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.4 Create RDDs from lists or text files\n",
        "text_rdd = sc.textFile(\"sample_text.txt\")"
      ],
      "metadata": {
        "id": "Z-6wNGwXyGcB"
      },
      "id": "Z-6wNGwXyGcB",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.5\tmap(), filter(), flatMap() transformations\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).collect())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Chw21bytYB",
        "outputId": "b8a9c1e2-4a61-400c-af30-5d57f443d2c2"
      },
      "id": "K5Chw21bytYB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apache', 'Spark', 'is', 'an', 'open-source', 'distributed', 'general-purpose', 'cluster-computing', 'framework.', 'It', 'provides', 'an', 'interface', 'for', 'programming', 'entire', 'clusters', 'with', 'implicit', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'is', 'designed', 'to', 'cover', 'a', 'wide', 'range', 'of', 'workloads', 'such', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'queries,', 'and', 'streaming.']\n",
            "['Apache', 'Spark', 'an', 'general-purpose', 'framework.', 'an', 'interface', 'programming', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'a', 'range', 'workloads', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'and', 'streaming.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.6\tcollect(), count(), take() actions\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).count())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).collect())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).take(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8MoLveczQ3L",
        "outputId": "ffe1da3b-5b0d-4c25-def8-1c4d595eb228"
      },
      "id": "_8MoLveczQ3L",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "['Apache', 'Spark', 'an', 'general-purpose', 'framework.', 'an', 'interface', 'programming', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'a', 'range', 'workloads', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'and', 'streaming.']\n",
            "['Apache', 'Spark', 'an']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.7\tCreate DataFrame from Python dictionary/list\n",
        "data_frame_data = [\n",
        "    {\"name\": \"Alice\", \"age\": 25},\n",
        "    {\"name\": \"Bob\", \"age\": 30},\n",
        "    {\"name\": \"Charlie\", \"surname:\":\"theron\",\"age\": 35}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data_frame_data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2Ss9B1X3ekm",
        "outputId": "ff84ff7d-de34-4f6c-ee10-927c7c30e51b"
      },
      "id": "y2Ss9B1X3ekm",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+--------+\n",
            "|age|   name|surname:|\n",
            "+---+-------+--------+\n",
            "| 25|  Alice|    NULL|\n",
            "| 30|    Bob|    NULL|\n",
            "| 35|Charlie|  theron|\n",
            "+---+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LPKPzWzR8DL",
        "outputId": "4036445a-aac9-460b-f5a5-a5384c792d23"
      },
      "id": "4LPKPzWzR8DL",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(age=25, name='Alice', surname:=None),\n",
              " Row(age=30, name='Bob', surname:=None),\n",
              " Row(age=35, name='Charlie', surname:='theron')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.8\tCreate DataFrame from CSV/JSON/Parquet\n",
        "csv_data = \"\"\"id,name,age,salary\n",
        "1,Alice,30,100000\n",
        "2,Bob,,85000\n",
        "3,Charlie,25,70000\n",
        "4,David,35,\n",
        "5,Eve,29,90000\n",
        "\"\"\"\n",
        "\n",
        "with open(\"sample_employees.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "df = spark.read.csv(\"sample_employees.csv\", header=True, inferSchema=True)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMzR-9nqI87q",
        "outputId": "ff4b5801-409f-4b7c-aaff-d1f6591b3579"
      },
      "id": "ZMzR-9nqI87q",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----+------+\n",
            "| id|   name| age|salary|\n",
            "+---+-------+----+------+\n",
            "|  1|  Alice|  30|100000|\n",
            "|  2|    Bob|NULL| 85000|\n",
            "|  3|Charlie|  25| 70000|\n",
            "|  4|  David|  35|  NULL|\n",
            "|  5|    Eve|  29| 90000|\n",
            "+---+-------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.9\tShow schema and data (printSchema(), show())\n",
        "print(df.printSchema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl4bZJZl5lkD",
        "outputId": "f59472bf-a977-442b-8070-e61657cf9ab4"
      },
      "id": "vl4bZJZl5lkD",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lTj3TniS_cx",
        "outputId": "3556bc5f-67a7-4e8e-8a7f-3d728f864eda"
      },
      "id": "4lTj3TniS_cx",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('id', IntegerType(), True), StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('salary', IntegerType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.10\tSelect specific columns\n",
        "df.select(\"name\",\"age\").show()"
      ],
      "metadata": {
        "id": "g7en7TvM6f7B",
        "outputId": "b1432853-e0d6-433b-883b-d533aa587346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "g7en7TvM6f7B",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|   name| age|\n",
            "+-------+----+\n",
            "|  Alice|  30|\n",
            "|    Bob|NULL|\n",
            "|Charlie|  25|\n",
            "|  David|  35|\n",
            "|    Eve|  29|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.11\tFilter rows using conditions\n",
        "test_lambda_func = lambda x: x-2\n",
        "df.select(\"name\",\"age\").filter(test_lambda_func(df.age)>25).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkfyjPy75v4g",
        "outputId": "3316621a-c993-43e2-d9d5-e895d35f3765"
      },
      "id": "GkfyjPy75v4g",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 30|\n",
            "|David| 35|\n",
            "|  Eve| 29|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.12\tRename columns\n",
        "df = df.withColumnRenamed(\"name\", \"full_name\")\n",
        "df.printSchema()\n",
        "\n",
        "new_column_names = [\"user_id\", \"full_name\",\"age\", \"salary net\"]\n",
        "df_renamed = df.toDF(*new_column_names)\n",
        "df_renamed.printSchema()"
      ],
      "metadata": {
        "id": "mtTNZdip6lJK",
        "outputId": "58e1d4a5-8815-4c5e-f6ef-1e89532e643f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mtTNZdip6lJK",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary net: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.13\tAdd new columns (withColumn)\n",
        "df = df.withColumn(\"age2\", df.age+2)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "kC_AnNCH8vIe",
        "outputId": "e9ca4ecc-0ee3-4c68-9fbd-9f375547e1b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kC_AnNCH8vIe",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- age2: integer (nullable = true)\n",
            "\n",
            "+---+---------+----+------+----+\n",
            "| id|full_name| age|salary|age2|\n",
            "+---+---------+----+------+----+\n",
            "|  1|    Alice|  30|100000|  32|\n",
            "|  2|      Bob|NULL| 85000|NULL|\n",
            "|  3|  Charlie|  25| 70000|  27|\n",
            "|  4|    David|  35|  NULL|  37|\n",
            "|  5|      Eve|  29| 90000|  31|\n",
            "+---+---------+----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.14\tDrop columns\n",
        "df = df.drop(\"age2\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "5NfoH37y9GJ6",
        "outputId": "a6aaa61c-0259-4e20-e141-aa5366cef6d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5NfoH37y9GJ6",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.15\tRegister DataFrame as a SQL temporary view\n",
        "#1.16\tRun simple SELECT queries with spark.sql()\n",
        "\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "result = spark.sql(\"SELECT full_name, age FROM people WHERE age > 26\")\n",
        "result.show()\n",
        "\n",
        "# createOrReplaceTempView: Visible only within the current Spark session\n",
        "# createGlobalTempView   : Visible across multiple Spark sessions and accessed with the prefix global_temp."
      ],
      "metadata": {
        "id": "YuQudJGE9KFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5eca852-02df-4b1e-9038-9324ff466511"
      },
      "id": "YuQudJGE9KFO",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|full_name|age|\n",
            "+---------+---+\n",
            "|    Alice| 30|\n",
            "|    David| 35|\n",
            "|      Eve| 29|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat, upper, lower, trim, col, lit\n",
        "#New Sample DF\n",
        "\n",
        "data = [(\" Alice \", \"Smith\",\"01-01-1996\"), (\" Bob \", \"Brown\",\"01-01-1996\"), (\" Cathy \", \"Johnson\",\"01-01-1996\")]\n",
        "columns = [\"first_name\", \"last_name\", \"dob\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "#2.1\tString functions (concat, upper, lower, trim)\n",
        "df = df.withColumn(\"first_name\", trim(col(\"first_name\")))\n",
        "df = df.withColumn(\"last_name\", trim(col(\"last_name\")))\n",
        "df = df.withColumn(\"Fullname\", concat(col(\"first_name\"),lit(\" \"),\"last_name\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUMm4GOqIe9G",
        "outputId": "beb061d4-3ae9-4878-f118-d255621e9461"
      },
      "id": "kUMm4GOqIe9G",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+\n",
            "|first_name|last_name|       dob|\n",
            "+----------+---------+----------+\n",
            "|    Alice |    Smith|01-01-1996|\n",
            "|      Bob |    Brown|01-01-1996|\n",
            "|    Cathy |  Johnson|01-01-1996|\n",
            "+----------+---------+----------+\n",
            "\n",
            "+----------+---------+----------+-------------+\n",
            "|first_name|last_name|       dob|     Fullname|\n",
            "+----------+---------+----------+-------------+\n",
            "|     Alice|    Smith|01-01-1996|  Alice Smith|\n",
            "|       Bob|    Brown|01-01-1996|    Bob Brown|\n",
            "|     Cathy|  Johnson|01-01-1996|Cathy Johnson|\n",
            "+----------+---------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col, datediff, current_date, floor\n",
        "#2.2\tDate/time functions (current_date, datediff, date_format)\n",
        "df = df.withColumn(\"dob\", to_date(col(\"dob\"), format= \"dd-mm-yyyy\"))\n",
        "df = df.withColumn(\"age\", floor(datediff(current_date(), col(\"dob\"))/365))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJlxAciIVU4q",
        "outputId": "823a3d82-bb42-4ea3-c332-6c4fedfdebf9"
      },
      "id": "HJlxAciIVU4q",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+\n",
            "|first_name|last_name|       dob|     Fullname|age|\n",
            "+----------+---------+----------+-------------+---+\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|\n",
            "+----------+---------+----------+-------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col, when\n",
        "#2.3\tConditional logic with when() and otherwise()\n",
        "df.withColumn(\"age2\",when(col(\"age\") > 25, \"y\").otherwise(\"n\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzGpIbI7bzoo",
        "outputId": "cfd11f79-63ef-425e-bea3-fbbd23fe86ca"
      },
      "id": "xzGpIbI7bzoo",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+----+\n",
            "|first_name|last_name|       dob|     Fullname|age|age2|\n",
            "+----------+---------+----------+-------------+---+----+\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|   y|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|   y|\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|   y|\n",
            "+----------+---------+----------+-------------+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.4\tgroupBy() with aggregation functions (count, avg, sum, max, min)\n",
        "df.groupBy(\"age\").agg({\"age\":\"count\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-CAmORQcBQd",
        "outputId": "b3318bfc-1b1d-4bb5-c65b-8a9a0acebacd"
      },
      "id": "X-CAmORQcBQd",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "|age|count(age)|\n",
            "+---+----------+\n",
            "| 29|         3|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.5\tMultiple aggregations in one statement\n",
        "df.groupBy(\"age\").agg({\"age\":\"count\",\"age\":\"avg\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7LqG5UrWZLy",
        "outputId": "52dd6bf3-b4b4-4ab7-85da-61d3a222ced8"
      },
      "id": "o7LqG5UrWZLy",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "|age|avg(age)|\n",
            "+---+--------+\n",
            "| 29|    29.0|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.6\torderBy() ascending/descending\n",
        "df.orderBy(col(\"first_name\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIcvvWZYcWSq",
        "outputId": "ac3b2c89-82c5-42e6-87ac-6d5727f5d007"
      },
      "id": "dIcvvWZYcWSq",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+\n",
            "|first_name|last_name|       dob|     Fullname|age|\n",
            "+----------+---------+----------+-------------+---+\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|\n",
            "+----------+---------+----------+-------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.7\tMulti-column ordering\n",
        "df.orderBy([col(\"last_name\").desc(),col(\"first_name\").desc()]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IehWIf50cgve",
        "outputId": "818e2fa5-f251-4fd6-93c8-e2157ca2166b"
      },
      "id": "IehWIf50cgve",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+\n",
            "|first_name|last_name|       dob|     Fullname|age|\n",
            "+----------+---------+----------+-------------+---+\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|\n",
            "+----------+---------+----------+-------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.8\tInner, left, right, full joins\n",
        "data1 = [\n",
        "    (1, \"Alice\", \"HR\"),\n",
        "    (2, \"Bob\", \"IT\"),\n",
        "    (3, \"Cathy\", \"Finance\"),\n",
        "    (4, \"David\", \"IT\")\n",
        "]\n",
        "columns1 = [\"emp_id\", \"name\", \"dept\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "df1.show()\n",
        "\n",
        "data2 = [\n",
        "    (1, 5000),\n",
        "    (2, 6000),\n",
        "    (4, 7000),\n",
        "    (5, 8000)\n",
        "]\n",
        "columns2 = [\"emp_id\", \"salary\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "df2.show()\n",
        "\n",
        "print(\"INNER JOIN\\n\")\n",
        "inner_join_df = df1.join(df2, on = \"emp_id\", how = \"inner\")\n",
        "inner_join_df.show()\n",
        "\n",
        "print(\"LEFT JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"left\").show()\n",
        "\n",
        "print(\"RIGHT JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"right\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYhMtM0fcqKn",
        "outputId": "62d0f597-589b-41a9-92c4-2d77ff225703"
      },
      "id": "PYhMtM0fcqKn",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+\n",
            "|emp_id| name|   dept|\n",
            "+------+-----+-------+\n",
            "|     1|Alice|     HR|\n",
            "|     2|  Bob|     IT|\n",
            "|     3|Cathy|Finance|\n",
            "|     4|David|     IT|\n",
            "+------+-----+-------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "+------+------+\n",
            "\n",
            "INNER JOIN\n",
            "\n",
            "+------+-----+----+------+\n",
            "|emp_id| name|dept|salary|\n",
            "+------+-----+----+------+\n",
            "|     1|Alice|  HR|  5000|\n",
            "|     2|  Bob|  IT|  6000|\n",
            "|     4|David|  IT|  7000|\n",
            "+------+-----+----+------+\n",
            "\n",
            "LEFT JOIN\n",
            "\n",
            "+------+-----+-------+------+\n",
            "|emp_id| name|   dept|salary|\n",
            "+------+-----+-------+------+\n",
            "|     1|Alice|     HR|  5000|\n",
            "|     2|  Bob|     IT|  6000|\n",
            "|     3|Cathy|Finance|  NULL|\n",
            "|     4|David|     IT|  7000|\n",
            "+------+-----+-------+------+\n",
            "\n",
            "RIGHT JOIN\n",
            "\n",
            "+------+-----+----+------+\n",
            "|emp_id| name|dept|salary|\n",
            "+------+-----+----+------+\n",
            "|     1|Alice|  HR|  5000|\n",
            "|     2|  Bob|  IT|  6000|\n",
            "|     5| NULL|NULL|  8000|\n",
            "|     4|David|  IT|  7000|\n",
            "+------+-----+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.9\tSemi and anti joins\n",
        "print(\"SEMI JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"left_semi\").show()\n",
        "print(\"ANTI JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"left_anti\").show()\n",
        "\n",
        "#2.1\tSelf joins\n",
        "print(\"SELF JOIN\\n\")\n",
        "df1.join(df1, on = \"emp_id\", how = \"inner\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4iW7uytdjKV",
        "outputId": "6c59b1d5-f0d1-4a9f-e674-047df6066243"
      },
      "id": "t4iW7uytdjKV",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEMI JOIN\n",
            "\n",
            "+------+-----+----+\n",
            "|emp_id| name|dept|\n",
            "+------+-----+----+\n",
            "|     1|Alice|  HR|\n",
            "|     2|  Bob|  IT|\n",
            "|     4|David|  IT|\n",
            "+------+-----+----+\n",
            "\n",
            "ANTI JOIN\n",
            "\n",
            "+------+-----+-------+\n",
            "|emp_id| name|   dept|\n",
            "+------+-----+-------+\n",
            "|     3|Cathy|Finance|\n",
            "+------+-----+-------+\n",
            "\n",
            "SELF JOIN\n",
            "\n",
            "+------+-----+-------+-----+-------+\n",
            "|emp_id| name|   dept| name|   dept|\n",
            "+------+-----+-------+-----+-------+\n",
            "|     1|Alice|     HR|Alice|     HR|\n",
            "|     2|  Bob|     IT|  Bob|     IT|\n",
            "|     3|Cathy|Finance|Cathy|Finance|\n",
            "|     4|David|     IT|David|     IT|\n",
            "+------+-----+-------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [\n",
        "    (1, \"Alice\", \"HR\"),\n",
        "    (2, \"Bob\", \"IT\"),\n",
        "    (3, \"Cathy\", \"Finance\"),\n",
        "    (4, \"David\", \"IT\"),\n",
        "    (5, \"Suther\", \"BYju\")\n",
        "]\n",
        "columns1 = [\"emp_id\", \"name\", \"dept\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "df1.show()\n",
        "\n",
        "data2 = [\n",
        "    (1, 5000),\n",
        "    (2, 6000),\n",
        "    (4, 7000),\n",
        "    (5, 8000),\n",
        "    (6, None)\n",
        "]\n",
        "columns2 = [\"emp_id\", \"salary\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CVc7bXefjMq",
        "outputId": "4fb3f5af-2db8-49b4-9438-141a1f0312b2"
      },
      "id": "6CVc7bXefjMq",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+\n",
            "|emp_id|  name|   dept|\n",
            "+------+------+-------+\n",
            "|     1| Alice|     HR|\n",
            "|     2|   Bob|     IT|\n",
            "|     3| Cathy|Finance|\n",
            "|     4| David|     IT|\n",
            "|     5|Suther|   BYju|\n",
            "+------+------+-------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "|     6|  NULL|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.11\tHandling nulls (fillna, dropna, na.replace)\n",
        "df2.fillna({\"salary\":10000}).show()\n",
        "df2.dropna().show()\n",
        "df2.fillna({\"salary\":10000}).na.replace(10000,20000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T70Ct0lkhG3_",
        "outputId": "894f6e80-4dc6-4051-ce7e-b1331149bfcf"
      },
      "id": "T70Ct0lkhG3_",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "|     6| 10000|\n",
            "+------+------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "+------+------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "|     6| 20000|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import equal_null\n",
        "#2.12\tReplace specific values in a column\n",
        "df2.replace(8000,20000).show()\n",
        "\n",
        "df2.na.replace({4:5},subset = [\"salary\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPooOuYohYaw",
        "outputId": "c6a8e420-0f72-4532-e263-43a981ed3b38"
      },
      "id": "xPooOuYohYaw",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5| 20000|\n",
            "|     6|  NULL|\n",
            "+------+------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "|     6|  NULL|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.13\tCache and uncache data\n",
        "df2.cache()\n",
        "df2.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0tzTP8HkbAU",
        "outputId": "0f5b65da-f63b-48bd-d8d7-433b88b6ce8a"
      },
      "id": "_0tzTP8HkbAU",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[emp_id: bigint, salary: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.14\tExplain storage levels\n",
        "df2.storageLevel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W006qGeUkdkJ",
        "outputId": "758d7249-605f-437a-a23c-381747aa9598"
      },
      "id": "W006qGeUkdkJ",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StorageLevel(False, False, False, False, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    (\"Alice\", \"Math\", 95),\n",
        "    (\"Bob\", \"Math\", 90),\n",
        "    (\"Charlie\", \"Math\", 90),\n",
        "    (\"David\", \"Math\", 85),\n",
        "    (\"Alice\", \"Physics\", 88),\n",
        "    (\"Bob\", \"Physics\", 92),\n",
        "    (\"Charlie\", \"Physics\", 85)\n",
        "]\n",
        "\n",
        "columns = [\"Name\", \"Subject\", \"Score\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jCqsLn-Gu_j",
        "outputId": "6cc31031-3513-47e4-c446-9e0017a13628"
      },
      "id": "2jCqsLn-Gu_j",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|   Name|Subject|Score|\n",
            "+-------+-------+-----+\n",
            "|  Alice|   Math|   95|\n",
            "|    Bob|   Math|   90|\n",
            "|Charlie|   Math|   90|\n",
            "|  David|   Math|   85|\n",
            "|  Alice|Physics|   88|\n",
            "|    Bob|Physics|   92|\n",
            "|Charlie|Physics|   85|\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1 Ranking functions (rank, dense_rank, row_number)\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, dense_rank, row_number\n",
        "\n",
        "windowSpec = Window.partitionBy(\"Subject\").orderBy(df.Score.desc())"
      ],
      "metadata": {
        "id": "iN5L_38yNpy_"
      },
      "id": "iN5L_38yNpy_",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "just_score_window = Window.orderBy(df.Score.desc())\n",
        "\n",
        "df.withColumn(\"rank\",rank().over(just_score_window)).show()\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(Window.partitionBy(df.Subject).orderBy(df.Score.desc()))).show()\n",
        "df.withColumn(\"row_number\",row_number().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYoamXaBN6wJ",
        "outputId": "415c5b68-ca2d-49ce-e632-da288ff164e5"
      },
      "id": "JYoamXaBN6wJ",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+----+\n",
            "|   Name|Subject|Score|rank|\n",
            "+-------+-------+-----+----+\n",
            "|  Alice|   Math|   95|   1|\n",
            "|    Bob|Physics|   92|   2|\n",
            "|    Bob|   Math|   90|   3|\n",
            "|Charlie|   Math|   90|   3|\n",
            "|  Alice|Physics|   88|   5|\n",
            "|  David|   Math|   85|   6|\n",
            "|Charlie|Physics|   85|   6|\n",
            "+-------+-------+-----+----+\n",
            "\n",
            "+-------+-------+-----+----------+\n",
            "|   Name|Subject|Score|dense_rank|\n",
            "+-------+-------+-----+----------+\n",
            "|  Alice|   Math|   95|         1|\n",
            "|    Bob|   Math|   90|         2|\n",
            "|Charlie|   Math|   90|         2|\n",
            "|  David|   Math|   85|         3|\n",
            "|    Bob|Physics|   92|         1|\n",
            "|  Alice|Physics|   88|         2|\n",
            "|Charlie|Physics|   85|         3|\n",
            "+-------+-------+-----+----------+\n",
            "\n",
            "+-------+-------+-----+----------+\n",
            "|   Name|Subject|Score|row_number|\n",
            "+-------+-------+-----+----------+\n",
            "|  Alice|   Math|   95|         1|\n",
            "|    Bob|   Math|   90|         2|\n",
            "|Charlie|   Math|   90|         3|\n",
            "|  David|   Math|   85|         4|\n",
            "|    Bob|Physics|   92|         1|\n",
            "|  Alice|Physics|   88|         2|\n",
            "|Charlie|Physics|   85|         3|\n",
            "+-------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2\tAggregations over a window (lead, lag, sum over partition)\n",
        "from pyspark.sql.functions import  lag, lead\n",
        "df.withColumn(\"prev_score\", lag(\"Score\").over(windowSpec)).show()\n",
        "df.withColumn(\"next_score\", lead(\"Score\").over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym6C_S91hFFz",
        "outputId": "804a7a40-1c6b-4263-f4f5-1b51ca590d61"
      },
      "id": "Ym6C_S91hFFz",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+----------+\n",
            "|   Name|Subject|Score|prev_score|\n",
            "+-------+-------+-----+----------+\n",
            "|  Alice|   Math|   95|      NULL|\n",
            "|    Bob|   Math|   90|        95|\n",
            "|Charlie|   Math|   90|        90|\n",
            "|  David|   Math|   85|        90|\n",
            "|    Bob|Physics|   92|      NULL|\n",
            "|  Alice|Physics|   88|        92|\n",
            "|Charlie|Physics|   85|        88|\n",
            "+-------+-------+-----+----------+\n",
            "\n",
            "+-------+-------+-----+----------+\n",
            "|   Name|Subject|Score|next_score|\n",
            "+-------+-------+-----+----------+\n",
            "|  Alice|   Math|   95|        90|\n",
            "|    Bob|   Math|   90|        90|\n",
            "|Charlie|   Math|   90|        85|\n",
            "|  David|   Math|   85|      NULL|\n",
            "|    Bob|Physics|   92|        88|\n",
            "|  Alice|Physics|   88|        85|\n",
            "|Charlie|Physics|   85|      NULL|\n",
            "+-------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.3\tBroadcast joins for small datasets\n",
        "from pyspark.sql.functions import broadcast\n",
        "df1.join(broadcast(df2), on = \"emp_id\", how = \"inner\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQRbDmLphq49",
        "outputId": "0fba7d2f-2539-49a4-89a1-3062af69affe"
      },
      "id": "HQRbDmLphq49",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----+------+\n",
            "|emp_id|  name|dept|salary|\n",
            "+------+------+----+------+\n",
            "|     1| Alice|  HR|  5000|\n",
            "|     2|   Bob|  IT|  6000|\n",
            "|     4| David|  IT|  7000|\n",
            "|     5|Suther|BYju|  8000|\n",
            "+------+------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.4\tCross joins\n",
        "df1.crossJoin(df2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHs51X2NiHCB",
        "outputId": "57ed75f3-42b2-445d-cd2d-ed1df273a518"
      },
      "id": "GHs51X2NiHCB",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+------+\n",
            "|emp_id|  name|   dept|emp_id|salary|\n",
            "+------+------+-------+------+------+\n",
            "|     1| Alice|     HR|     1|  5000|\n",
            "|     1| Alice|     HR|     2|  6000|\n",
            "|     2|   Bob|     IT|     1|  5000|\n",
            "|     2|   Bob|     IT|     2|  6000|\n",
            "|     1| Alice|     HR|     4|  7000|\n",
            "|     1| Alice|     HR|     5|  8000|\n",
            "|     1| Alice|     HR|     6|  NULL|\n",
            "|     2|   Bob|     IT|     4|  7000|\n",
            "|     2|   Bob|     IT|     5|  8000|\n",
            "|     2|   Bob|     IT|     6|  NULL|\n",
            "|     3| Cathy|Finance|     1|  5000|\n",
            "|     3| Cathy|Finance|     2|  6000|\n",
            "|     4| David|     IT|     1|  5000|\n",
            "|     4| David|     IT|     2|  6000|\n",
            "|     5|Suther|   BYju|     1|  5000|\n",
            "|     5|Suther|   BYju|     2|  6000|\n",
            "|     3| Cathy|Finance|     4|  7000|\n",
            "|     3| Cathy|Finance|     5|  8000|\n",
            "|     3| Cathy|Finance|     6|  NULL|\n",
            "|     4| David|     IT|     4|  7000|\n",
            "+------+------+-------+------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHO6Xc9cjdnB",
        "outputId": "f6eef19c-ffef-4a83-a76c-1956687b25b1"
      },
      "id": "nHO6Xc9cjdnB",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|   Name|Subject|Score|\n",
            "+-------+-------+-----+\n",
            "|  Alice|   Math|   95|\n",
            "|    Bob|   Math|   90|\n",
            "|Charlie|   Math|   90|\n",
            "|  David|   Math|   85|\n",
            "|  Alice|Physics|   88|\n",
            "|    Bob|Physics|   92|\n",
            "|Charlie|Physics|   85|\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.5\tCreate and register Python UDFs\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "double_udf = udf(lambda x:x*2, IntegerType())\n",
        "df.withColumn(\"newCol\",double_udf(\"Score\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnNCzT-5jBEC",
        "outputId": "a6513ddf-a934-4aa5-9816-b273875d1ab3"
      },
      "id": "dnNCzT-5jBEC",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+------+\n",
            "|   Name|Subject|Score|newCol|\n",
            "+-------+-------+-----+------+\n",
            "|  Alice|   Math|   95|   190|\n",
            "|    Bob|   Math|   90|   180|\n",
            "|Charlie|   Math|   90|   180|\n",
            "|  David|   Math|   85|   170|\n",
            "|  Alice|Physics|   88|   176|\n",
            "|    Bob|Physics|   92|   184|\n",
            "|Charlie|Physics|   85|   170|\n",
            "+-------+-------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.7\tExplode arrays and maps\n",
        "from pyspark.sql.functions import explode\n",
        "data = [\n",
        "    (1, [\"a\", \"b\", \"c\"]),\n",
        "    (2, [\"x\", \"y\"])\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"letters\"])\n",
        "df.show()\n",
        "df_exploded = df.withColumn(\"letter\", explode(\"letters\"))\n",
        "df_exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0EnjgGSusdL",
        "outputId": "4965c0f4-8d64-482f-9183-aa1344ae10a6"
      },
      "id": "t0EnjgGSusdL",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+\n",
            "| id|  letters|\n",
            "+---+---------+\n",
            "|  1|[a, b, c]|\n",
            "|  2|   [x, y]|\n",
            "+---+---------+\n",
            "\n",
            "+---+---------+------+\n",
            "| id|  letters|letter|\n",
            "+---+---------+------+\n",
            "|  1|[a, b, c]|     a|\n",
            "|  1|[a, b, c]|     b|\n",
            "|  1|[a, b, c]|     c|\n",
            "|  2|   [x, y]|     x|\n",
            "|  2|   [x, y]|     y|\n",
            "+---+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1\tWrite data with partitioning & bucketing\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"department\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Alice\", \"HR\", 5000),\n",
        "    (2, \"Bob\", \"IT\", 6000),\n",
        "    (3, \"Charlie\", \"IT\", 7000),\n",
        "    (4, \"David\", \"HR\", 5500),\n",
        "    (5, \"Eve\", \"Finance\", 6500),\n",
        "    (6, \"Frank\", \"Finance\", 6000),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TJ_kfMmwXiY",
        "outputId": "20ab345e-d79b-4ae1-8a04-56659a4f2e0b"
      },
      "id": "8TJ_kfMmwXiY",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------+------+\n",
            "| id|   name|department|salary|\n",
            "+---+-------+----------+------+\n",
            "|  1|  Alice|        HR|  5000|\n",
            "|  2|    Bob|        IT|  6000|\n",
            "|  3|Charlie|        IT|  7000|\n",
            "|  4|  David|        HR|  5500|\n",
            "|  5|    Eve|   Finance|  6500|\n",
            "|  6|  Frank|   Finance|  6000|\n",
            "+---+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").partitionBy(\"department\").parquet(\"/content/employee_partitioned\")"
      ],
      "metadata": {
        "id": "9fCb1OW_xG3w"
      },
      "id": "9fCb1OW_xG3w",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\") \\\n",
        "    .bucketBy(3, \"department\") \\\n",
        "    .sortBy(\"salary\") \\\n",
        "    .saveAsTable(\"employee_bucketed\")"
      ],
      "metadata": {
        "id": "UTHFj7HCxp9U"
      },
      "id": "UTHFj7HCxp9U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.explain()"
      ],
      "metadata": {
        "id": "XTKl3P3a1PVW"
      },
      "id": "XTKl3P3a1PVW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.1 Data preprocessing (VectorAssembler, StringIndexer, OneHotEncoder)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Start Spark\n",
        "spark = SparkSession.builder.appName(\"Preprocessing\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [(0, \"red\"), (1, \"blue\"), (2, \"green\"), (3, \"blue\")]\n",
        "df = spark.createDataFrame(data, [\"id\", \"color\"])\n",
        "df.show()\n",
        "df_indexed = StringIndexer(inputCol=\"color\", outputCol=\"color_index\").fit(df).transform(df)\n",
        "df_indexed.show()"
      ],
      "metadata": {
        "id": "zMjrz9iw7viQ"
      },
      "id": "zMjrz9iw7viQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(inputCol=\"color_index\", outputCol=\"color_vec\")\n",
        "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
        "df_encoded.show()"
      ],
      "metadata": {
        "id": "9riVqgsm8U5e"
      },
      "id": "9riVqgsm8U5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"id\", \"color_vec\"],  # numeric + encoded columns\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "df_final = assembler.transform(df_encoded)\n",
        "df_final.select(\"features\").show(truncate=False)"
      ],
      "metadata": {
        "id": "kf2t5c3F9A-x"
      },
      "id": "kf2t5c3F9A-x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o8ySOKaZ8ZHZ"
      },
      "id": "o8ySOKaZ8ZHZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:Torch]",
      "language": "python",
      "name": "conda-env-Torch-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}