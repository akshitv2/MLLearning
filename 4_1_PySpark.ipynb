{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-08-10T14:18:55.361917Z",
          "start_time": "2025-08-10T14:18:48.012989Z"
        },
        "id": "initial_id"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from sklearn import datasets\n",
        "import time\n",
        "\n",
        "#1.2 Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SimModeExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing Configs\n",
        "for k, v in spark.sparkContext.getConf().getAll():\n",
        "    print(f\"{k} = {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEmm_yFKcz0W",
        "outputId": "7dcfa68c-f0b1-46d2-bc24-c05a9baaf838"
      },
      "id": "DEmm_yFKcz0W",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.app.id = local-1755073533927\n",
            "spark.executor.id = driver\n",
            "spark.app.startTime = 1755073529975\n",
            "spark.rdd.compress = True\n",
            "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.serializer.objectStreamReset = 100\n",
            "spark.app.submitTime = 1755073529594\n",
            "spark.master = local[*]\n",
            "spark.submit.pyFiles = \n",
            "spark.submit.deployMode = client\n",
            "spark.app.name = SimModeExample\n",
            "spark.driver.port = 36121\n",
            "spark.driver.host = b70c4d339e42\n",
            "spark.ui.showConsoleProgress = true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"Apache Spark is an open-source distributed general-purpose cluster-computing framework.\n",
        "It provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\n",
        "Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries, and streaming.\"\"\"\n",
        "\n",
        "with open(\"sample_text.txt\", \"w\") as f:\n",
        "    f.write(sample_text)"
      ],
      "metadata": {
        "id": "XRGAKBTaigSI"
      },
      "id": "XRGAKBTaigSI",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.4 Create RDDs from lists or text files\n",
        "text_rdd = sc.textFile(\"sample_test.txt\")"
      ],
      "metadata": {
        "id": "Z-6wNGwXyGcB"
      },
      "id": "Z-6wNGwXyGcB",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.5\tmap(), filter(), flatMap() transformations\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).collect())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Chw21bytYB",
        "outputId": "63d7f4ef-1984-4bc8-8b14-2c0b1f75c221"
      },
      "id": "K5Chw21bytYB",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apache', 'Spark', 'is', 'an', 'open-source', 'distributed', 'general-purpose', 'cluster-computing', 'framework.', 'It', 'provides', 'an', 'interface', 'for', 'programming', 'entire', 'clusters', 'with', 'implicit', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'is', 'designed', 'to', 'cover', 'a', 'wide', 'range', 'of', 'workloads', 'such', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'queries,', 'and', 'streaming.']\n",
            "['Apache', 'Spark', 'an', 'general-purpose', 'framework.', 'an', 'interface', 'programming', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'a', 'range', 'workloads', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'and', 'streaming.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.6\tcollect(), count(), take() actions\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).count())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).collect())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).take(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8MoLveczQ3L",
        "outputId": "fa138e3d-5807-4b7f-b75e-6c0968e59046"
      },
      "id": "_8MoLveczQ3L",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "['Apache', 'Spark', 'an', 'general-purpose', 'framework.', 'an', 'interface', 'programming', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'a', 'range', 'workloads', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'and', 'streaming.']\n",
            "['Apache', 'Spark', 'an']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.7\tCreate DataFrame from Python dictionary/list\n",
        "data_frame_data = [\n",
        "    {\"name\": \"Alice\", \"age\": 25},\n",
        "    {\"name\": \"Bob\", \"age\": 30},\n",
        "    {\"name\": \"Charlie\", \"surname:\":\"theron\",\"age\": 35}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data_frame_data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2Ss9B1X3ekm",
        "outputId": "f4dfaec6-33f7-47e2-bc25-a2ffefdb0f78"
      },
      "id": "y2Ss9B1X3ekm",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+--------+\n",
            "|age|   name|surname:|\n",
            "+---+-------+--------+\n",
            "| 25|  Alice|    NULL|\n",
            "| 30|    Bob|    NULL|\n",
            "| 35|Charlie|  theron|\n",
            "+---+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.8\tCreate DataFrame from CSV/JSON/Parquet\n",
        "csv_data = \"\"\"id,name,age,salary\n",
        "1,Alice,30,100000\n",
        "2,Bob,,85000\n",
        "3,Charlie,25,70000\n",
        "4,David,35,\n",
        "5,Eve,29,90000\n",
        "\"\"\"\n",
        "\n",
        "with open(\"sample_employees.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "df = spark.read.csv(\"sample_employees.csv\", header=True, inferSchema=True)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMzR-9nqI87q",
        "outputId": "89214542-ff9f-4e14-b916-d7f0f27f78b9"
      },
      "id": "ZMzR-9nqI87q",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----+------+\n",
            "| id|   name| age|salary|\n",
            "+---+-------+----+------+\n",
            "|  1|  Alice|  30|100000|\n",
            "|  2|    Bob|NULL| 85000|\n",
            "|  3|Charlie|  25| 70000|\n",
            "|  4|  David|  35|  NULL|\n",
            "|  5|    Eve|  29| 90000|\n",
            "+---+-------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.9\tShow schema and data (printSchema(), show())\n",
        "print(df.printSchema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl4bZJZl5lkD",
        "outputId": "2abfd71c-a11b-4b00-dbbf-c1b572960532"
      },
      "id": "vl4bZJZl5lkD",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.10\tSelect specific columns\n",
        "df.select(\"name\",\"age\").show()"
      ],
      "metadata": {
        "id": "g7en7TvM6f7B",
        "outputId": "46d0cdfa-85dd-4d92-d814-7223fb2d819d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "g7en7TvM6f7B",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|   name| age|\n",
            "+-------+----+\n",
            "|  Alice|  30|\n",
            "|    Bob|NULL|\n",
            "|Charlie|  25|\n",
            "|  David|  35|\n",
            "|    Eve|  29|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.11\tFilter rows using conditions\n",
        "test_lambda_func = lambda x: x-2\n",
        "df.select(\"name\",\"age\").filter(test_lambda_func(df.age)>25).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkfyjPy75v4g",
        "outputId": "e9dfbf14-3fbb-4df0-ef28-a0868713c6dc"
      },
      "id": "GkfyjPy75v4g",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 30|\n",
            "|David| 35|\n",
            "|  Eve| 29|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.12\tRename columns\n",
        "df = df.withColumnRenamed(\"name\", \"full_name\")\n",
        "df.printSchema()\n",
        "\n",
        "new_column_names = [\"user_id\", \"full_name\",\"age\", \"salary net\"]\n",
        "df_renamed = df.toDF(*new_column_names)\n",
        "df_renamed.printSchema()"
      ],
      "metadata": {
        "id": "mtTNZdip6lJK",
        "outputId": "59823592-57da-42cc-baff-fc744b8b2892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mtTNZdip6lJK",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary net: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.13\tAdd new columns (withColumn)\n",
        "df = df.withColumn(\"age2\", df.age+2)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "kC_AnNCH8vIe",
        "outputId": "2b9fcd77-9068-490a-c1cd-1708675d68fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kC_AnNCH8vIe",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- age2: integer (nullable = true)\n",
            "\n",
            "+---+-------+----+------+----+\n",
            "| id|   name| age|salary|age2|\n",
            "+---+-------+----+------+----+\n",
            "|  1|  Alice|  30|100000|  32|\n",
            "|  2|    Bob|NULL| 85000|NULL|\n",
            "|  3|Charlie|  25| 70000|  27|\n",
            "|  4|  David|  35|  NULL|  37|\n",
            "|  5|    Eve|  29| 90000|  31|\n",
            "+---+-------+----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.14\tDrop columns\n",
        "df = df.drop(\"age2\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "5NfoH37y9GJ6",
        "outputId": "ebaeacdd-27bf-4840-9a70-672bfa038e28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5NfoH37y9GJ6",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YuQudJGE9KFO"
      },
      "id": "YuQudJGE9KFO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load text file into RDD\n",
        "text_rdd = sc.textFile(\"sample_text.txt\")\n",
        "\n",
        "def top_words_no_cache():\n",
        "    word_counts = (\n",
        "        text_rdd\n",
        "        .flatMap(lambda line: line.split())\n",
        "        .map(lambda w: (w.lower().strip(\".,!?\"), 1))\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "    )\n",
        "    top_10 = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
        "    return top_10\n",
        "\n",
        "def top_words_with_cache():\n",
        "    cached_rdd = (\n",
        "        text_rdd\n",
        "        .flatMap(lambda line: line.split())\n",
        "        .map(lambda w: (w.lower().strip(\".,!?\"), 1))\n",
        "        .cache()  # Cache the intermediate RDD\n",
        "    )\n",
        "    word_counts = cached_rdd.reduceByKey(lambda a, b: a + b)\n",
        "    top_10 = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
        "    return top_10\n",
        "\n",
        "# Measure time without caching\n",
        "start = time.time()\n",
        "result_no_cache = top_words_no_cache()\n",
        "time_no_cache = time.time() - start\n",
        "\n",
        "# Measure time with caching\n",
        "start = time.time()\n",
        "result_with_cache = top_words_with_cache()\n",
        "time_with_cache = time.time() - start\n",
        "\n",
        "# Print results\n",
        "print(\"\\nTop 10 words without caching:\")\n",
        "for word, count in result_no_cache:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\nTime taken without caching: {time_no_cache:.4f} seconds\")\n",
        "\n",
        "print(\"\\nTop 10 words with caching:\")\n",
        "for word, count in result_with_cache:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\nTime taken with caching: {time_with_cache:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTjmZLkKmvty",
        "outputId": "5c314a9a-9b96-489b-8245-d8f71b2636ca"
      },
      "id": "PTjmZLkKmvty",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 words without caching:\n",
            "an: 2\n",
            "and: 2\n",
            "spark: 2\n",
            "is: 2\n",
            "apache: 1\n",
            "open-source: 1\n",
            "distributed: 1\n",
            "cluster-computing: 1\n",
            "framework: 1\n",
            "it: 1\n",
            "\n",
            "Time taken without caching: 1.1633 seconds\n",
            "\n",
            "Top 10 words with caching:\n",
            "an: 2\n",
            "and: 2\n",
            "spark: 2\n",
            "is: 2\n",
            "apache: 1\n",
            "open-source: 1\n",
            "distributed: 1\n",
            "cluster-computing: 1\n",
            "framework: 1\n",
            "it: 1\n",
            "\n",
            "Time taken with caching: 1.3649 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Map Reduce Word Frequency\n",
        "text_rdd = spark.sparkContext.parallelize([\n",
        "    \"PySpark is great\",\n",
        "    \"PySpark runs locally\",\n",
        "    \"Word count is a classic example\"\n",
        "])\n",
        "\n",
        "# Word count\n",
        "word_counts = (\n",
        "    text_rdd\n",
        "    .flatMap(lambda line: line.split(\" \"))\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "print(\"\\n=== Word Count ===\")\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3pL2GW-iDNC",
        "outputId": "9fb060ca-3fdd-4a74-b066-22a1a16b9c2d"
      },
      "id": "e3pL2GW-iDNC",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Word Count ===\n",
            "PySpark: 2\n",
            "runs: 1\n",
            "Word: 1\n",
            "is: 2\n",
            "great: 1\n",
            "locally: 1\n",
            "count: 1\n",
            "a: 1\n",
            "classic: 1\n",
            "example: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd = sc.textFile(\"sample_text.txt\")\n",
        "text_rdd.map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-nNprHbiMgT",
        "outputId": "7b1a062c-7be8-4049-cadb-f2e205aa65cd"
      },
      "id": "K-nNprHbiMgT",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.',\n",
              "  1),\n",
              " ('Apache Spark is an open-source distributed general-purpose cluster-computing framework.',\n",
              "  1),\n",
              " ('Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries, and streaming.',\n",
              "  1)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd.flatMap(lambda sent: sent.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a,b:a+b).collect()"
      ],
      "metadata": {
        "id": "6VHdKuWNijsZ"
      },
      "id": "6VHdKuWNijsZ",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd.flatMap(lambda sent: sent.split(\" \")).filter(lambda x: \"a\" in x).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCXZ8EDSlZWj",
        "outputId": "197fad45-172c-46b8-9249-a88aff5df1df"
      },
      "id": "PCXZ8EDSlZWj",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apache',\n",
              " 'Spark',\n",
              " 'an',\n",
              " 'general-purpose',\n",
              " 'framework.',\n",
              " 'an',\n",
              " 'interface',\n",
              " 'programming',\n",
              " 'data',\n",
              " 'parallelism',\n",
              " 'and',\n",
              " 'fault-tolerance.',\n",
              " 'Spark',\n",
              " 'a',\n",
              " 'range',\n",
              " 'workloads',\n",
              " 'as',\n",
              " 'batch',\n",
              " 'applications,',\n",
              " 'iterative',\n",
              " 'algorithms,',\n",
              " 'interactive',\n",
              " 'and',\n",
              " 'streaming.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"sample_employees.csv\", header=True, inferSchema=True)\n",
        "\n",
        "print(\"Original Data:\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TW4q7szJH6Y",
        "outputId": "6333dfab-2f12-431d-8992-414bb3458c23"
      },
      "id": "8TW4q7szJH6Y",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "+---+-------+----+------+\n",
            "| id|   name| age|salary|\n",
            "+---+-------+----+------+\n",
            "|  1|  Alice|  30|100000|\n",
            "|  2|    Bob|NULL| 85000|\n",
            "|  3|Charlie|  25| 70000|\n",
            "|  4|  David|  35|  NULL|\n",
            "|  5|    Eve|  29| 90000|\n",
            "+---+-------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-10T14:19:28.924529Z",
          "start_time": "2025-08-10T14:19:25.057372Z"
        },
        "id": "1cbe3d220d7a182d"
      },
      "cell_type": "code",
      "source": [
        "dataset_iris = datasets.load_iris()\n",
        "spdf = spark.createDataFrame(dataset_iris.data, schema = dataset_iris.feature_names)"
      ],
      "id": "1cbe3d220d7a182d",
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-10T14:19:51.369672Z",
          "start_time": "2025-08-10T14:19:33.233690Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2b8261243ddb6d",
        "outputId": "287c4ede-a180-4caf-82d9-631a55871542"
      },
      "cell_type": "code",
      "source": [
        "spdf.head(5)"
      ],
      "id": "ae2b8261243ddb6d",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(sepal length (cm)=5.1, sepal width (cm)=3.5, petal length (cm)=1.4, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=4.9, sepal width (cm)=3.0, petal length (cm)=1.4, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=4.7, sepal width (cm)=3.2, petal length (cm)=1.3, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=4.6, sepal width (cm)=3.1, petal length (cm)=1.5, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=5.0, sepal width (cm)=3.6, petal length (cm)=1.4, petal width (cm)=0.2)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "564503e2bcfc66fa"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "564503e2bcfc66fa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:Torch]",
      "language": "python",
      "name": "conda-env-Torch-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}