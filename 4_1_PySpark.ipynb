{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-08-10T14:18:55.361917Z",
          "start_time": "2025-08-10T14:18:48.012989Z"
        },
        "id": "initial_id"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from sklearn import datasets\n",
        "import time\n",
        "\n",
        "#1.2 Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SimModeExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing Configs\n",
        "for k, v in spark.sparkContext.getConf().getAll():\n",
        "    print(f\"{k} = {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEmm_yFKcz0W",
        "outputId": "17877f8f-9b6f-4101-87b9-66f5a680c7d1"
      },
      "id": "DEmm_yFKcz0W",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark.app.submitTime = 1755099849606\n",
            "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.executor.id = driver\n",
            "spark.driver.host = 5cdbd42e068a\n",
            "spark.app.id = local-1755099851969\n",
            "spark.rdd.compress = True\n",
            "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.driver.port = 42165\n",
            "spark.serializer.objectStreamReset = 100\n",
            "spark.master = local[*]\n",
            "spark.submit.pyFiles = \n",
            "spark.submit.deployMode = client\n",
            "spark.app.name = SimModeExample\n",
            "spark.ui.showConsoleProgress = true\n",
            "spark.app.startTime = 1755099849863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"Apache Spark is an open-source distributed general-purpose cluster-computing framework.\n",
        "It provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\n",
        "Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries, and streaming.\"\"\"\n",
        "\n",
        "with open(\"sample_text.txt\", \"w\") as f:\n",
        "    f.write(sample_text)"
      ],
      "metadata": {
        "id": "XRGAKBTaigSI"
      },
      "id": "XRGAKBTaigSI",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.4 Create RDDs from lists or text files\n",
        "text_rdd = sc.textFile(\"sample_text.txt\")"
      ],
      "metadata": {
        "id": "Z-6wNGwXyGcB"
      },
      "id": "Z-6wNGwXyGcB",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.5\tmap(), filter(), flatMap() transformations\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).collect())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Chw21bytYB",
        "outputId": "627bec63-e895-42ab-a1a6-2adf08cf85b4"
      },
      "id": "K5Chw21bytYB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apache', 'Spark', 'is', 'an', 'open-source', 'distributed', 'general-purpose', 'cluster-computing', 'framework.', 'It', 'provides', 'an', 'interface', 'for', 'programming', 'entire', 'clusters', 'with', 'implicit', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'is', 'designed', 'to', 'cover', 'a', 'wide', 'range', 'of', 'workloads', 'such', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'queries,', 'and', 'streaming.']\n",
            "['Apache', 'Spark', 'an', 'general-purpose', 'framework.', 'an', 'interface', 'programming', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'a', 'range', 'workloads', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'and', 'streaming.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.6\tcollect(), count(), take() actions\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).count())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).collect())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).take(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8MoLveczQ3L",
        "outputId": "814b025d-ac1f-4efe-bf80-0c5046416d44"
      },
      "id": "_8MoLveczQ3L",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "['Apache', 'Spark', 'an', 'general-purpose', 'framework.', 'an', 'interface', 'programming', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'a', 'range', 'workloads', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'and', 'streaming.']\n",
            "['Apache', 'Spark', 'an']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.7\tCreate DataFrame from Python dictionary/list\n",
        "data_frame_data = [\n",
        "    {\"name\": \"Alice\", \"age\": 25},\n",
        "    {\"name\": \"Bob\", \"age\": 30},\n",
        "    {\"name\": \"Charlie\", \"surname:\":\"theron\",\"age\": 35}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data_frame_data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2Ss9B1X3ekm",
        "outputId": "596e0f13-ee04-4533-940f-2b433d9400be"
      },
      "id": "y2Ss9B1X3ekm",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+--------+\n",
            "|age|   name|surname:|\n",
            "+---+-------+--------+\n",
            "| 25|  Alice|    NULL|\n",
            "| 30|    Bob|    NULL|\n",
            "| 35|Charlie|  theron|\n",
            "+---+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LPKPzWzR8DL",
        "outputId": "ce98663d-19cc-4b19-dcf3-5014ca64f966"
      },
      "id": "4LPKPzWzR8DL",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Name='Alice', Subject='Math', Score=95),\n",
              " Row(Name='Bob', Subject='Math', Score=90),\n",
              " Row(Name='Charlie', Subject='Math', Score=90),\n",
              " Row(Name='David', Subject='Math', Score=85),\n",
              " Row(Name='Alice', Subject='Physics', Score=88),\n",
              " Row(Name='Bob', Subject='Physics', Score=92),\n",
              " Row(Name='Charlie', Subject='Physics', Score=85)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.8\tCreate DataFrame from CSV/JSON/Parquet\n",
        "csv_data = \"\"\"id,name,age,salary\n",
        "1,Alice,30,100000\n",
        "2,Bob,,85000\n",
        "3,Charlie,25,70000\n",
        "4,David,35,\n",
        "5,Eve,29,90000\n",
        "\"\"\"\n",
        "\n",
        "with open(\"sample_employees.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "df = spark.read.csv(\"sample_employees.csv\", header=True, inferSchema=True)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMzR-9nqI87q",
        "outputId": "14c5148f-a9e3-45dd-f22e-f174b115fa20"
      },
      "id": "ZMzR-9nqI87q",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----+------+\n",
            "| id|   name| age|salary|\n",
            "+---+-------+----+------+\n",
            "|  1|  Alice|  30|100000|\n",
            "|  2|    Bob|NULL| 85000|\n",
            "|  3|Charlie|  25| 70000|\n",
            "|  4|  David|  35|  NULL|\n",
            "|  5|    Eve|  29| 90000|\n",
            "+---+-------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.9\tShow schema and data (printSchema(), show())\n",
        "print(df.printSchema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl4bZJZl5lkD",
        "outputId": "fb5824b2-28e8-4600-850a-b62c68ca939b"
      },
      "id": "vl4bZJZl5lkD",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lTj3TniS_cx",
        "outputId": "f27808a8-3360-427b-9f65-b1ca23f86654"
      },
      "id": "4lTj3TniS_cx",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('Name', StringType(), True), StructField('Subject', StringType(), True), StructField('Score', LongType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.10\tSelect specific columns\n",
        "df.select(\"name\",\"age\").show()"
      ],
      "metadata": {
        "id": "g7en7TvM6f7B",
        "outputId": "c5d9753d-b155-410c-faa1-5ef09be9e31a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "g7en7TvM6f7B",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|   name| age|\n",
            "+-------+----+\n",
            "|  Alice|  30|\n",
            "|    Bob|NULL|\n",
            "|Charlie|  25|\n",
            "|  David|  35|\n",
            "|    Eve|  29|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.11\tFilter rows using conditions\n",
        "test_lambda_func = lambda x: x-2\n",
        "df.select(\"name\",\"age\").filter(test_lambda_func(df.age)>25).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkfyjPy75v4g",
        "outputId": "4cccd9ec-6ccb-4c65-fd5a-1d3abd7fcab1"
      },
      "id": "GkfyjPy75v4g",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 30|\n",
            "|David| 35|\n",
            "|  Eve| 29|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.12\tRename columns\n",
        "df = df.withColumnRenamed(\"name\", \"full_name\")\n",
        "df.printSchema()\n",
        "\n",
        "new_column_names = [\"user_id\", \"full_name\",\"age\", \"salary net\"]\n",
        "df_renamed = df.toDF(*new_column_names)\n",
        "df_renamed.printSchema()"
      ],
      "metadata": {
        "id": "mtTNZdip6lJK",
        "outputId": "e0470be2-724b-4e3d-ee3d-787843c3c18e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "id": "mtTNZdip6lJK",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- Subject: string (nullable = true)\n",
            " |-- Score: long (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: The number of columns doesn't match.\nOld column names (3): full_name, Subject, Score\nNew column names (4): user_id, full_name, age, salary net",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3778100625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_column_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"full_name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"salary net\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_renamed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_column_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_renamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   5488\u001b[0m                     \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cols\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m                 )\n\u001b[0;32m-> 5490\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The number of columns doesn't match.\nOld column names (3): full_name, Subject, Score\nNew column names (4): user_id, full_name, age, salary net"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.13\tAdd new columns (withColumn)\n",
        "df = df.withColumn(\"age2\", df.age+2)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "kC_AnNCH8vIe",
        "outputId": "d6a2bc10-29cc-487f-9dbc-c465e55e7778",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kC_AnNCH8vIe",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- age2: integer (nullable = true)\n",
            "\n",
            "+---+---------+----+------+----+\n",
            "| id|full_name| age|salary|age2|\n",
            "+---+---------+----+------+----+\n",
            "|  1|    Alice|  30|100000|  32|\n",
            "|  2|      Bob|NULL| 85000|NULL|\n",
            "|  3|  Charlie|  25| 70000|  27|\n",
            "|  4|    David|  35|  NULL|  37|\n",
            "|  5|      Eve|  29| 90000|  31|\n",
            "+---+---------+----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.14\tDrop columns\n",
        "df = df.drop(\"age2\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "5NfoH37y9GJ6",
        "outputId": "d10658a0-c73d-45f3-8303-d3c0939b69e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5NfoH37y9GJ6",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.15\tRegister DataFrame as a SQL temporary view\n",
        "#1.16\tRun simple SELECT queries with spark.sql()\n",
        "\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "result = spark.sql(\"SELECT full_name, age FROM people WHERE age > 26\")\n",
        "result.show()\n",
        "\n",
        "# createOrReplaceTempView: Visible only within the current Spark session\n",
        "# createGlobalTempView   : Visible across multiple Spark sessions and accessed with the prefix global_temp."
      ],
      "metadata": {
        "id": "YuQudJGE9KFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a26812c-afac-41c9-d92d-82a945d8613a"
      },
      "id": "YuQudJGE9KFO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|full_name|age|\n",
            "+---------+---+\n",
            "|    Alice| 30|\n",
            "|    David| 35|\n",
            "|      Eve| 29|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat, upper, lower, trim, col, lit\n",
        "#New Sample DF\n",
        "\n",
        "data = [(\" Alice \", \"Smith\",\"01-01-1996\"), (\" Bob \", \"Brown\",\"01-01-1996\"), (\" Cathy \", \"Johnson\",\"01-01-1996\")]\n",
        "columns = [\"first_name\", \"last_name\", \"dob\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "#2.1\tString functions (concat, upper, lower, trim)\n",
        "df = df.withColumn(\"first_name\", trim(col(\"first_name\")))\n",
        "df = df.withColumn(\"last_name\", trim(col(\"last_name\")))\n",
        "df = df.withColumn(\"Fullname\", concat(col(\"first_name\"),lit(\" \"),\"last_name\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUMm4GOqIe9G",
        "outputId": "1569573c-d2e7-40b7-f115-f5074c9be3e9"
      },
      "id": "kUMm4GOqIe9G",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+\n",
            "|first_name|last_name|       dob|\n",
            "+----------+---------+----------+\n",
            "|    Alice |    Smith|01-01-1996|\n",
            "|      Bob |    Brown|01-01-1996|\n",
            "|    Cathy |  Johnson|01-01-1996|\n",
            "+----------+---------+----------+\n",
            "\n",
            "+----------+---------+----------+-------------+\n",
            "|first_name|last_name|       dob|     Fullname|\n",
            "+----------+---------+----------+-------------+\n",
            "|     Alice|    Smith|01-01-1996|  Alice Smith|\n",
            "|       Bob|    Brown|01-01-1996|    Bob Brown|\n",
            "|     Cathy|  Johnson|01-01-1996|Cathy Johnson|\n",
            "+----------+---------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col, datediff, current_date, floor\n",
        "#2.2\tDate/time functions (current_date, datediff, date_format)\n",
        "df = df.withColumn(\"dob\", to_date(col(\"dob\"), format= \"dd-mm-yyyy\"))\n",
        "df = df.withColumn(\"age\", floor(datediff(current_date(), col(\"dob\"))/365))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJlxAciIVU4q",
        "outputId": "ea991014-9b8f-4bc0-e39d-fe0ba59bd671"
      },
      "id": "HJlxAciIVU4q",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+\n",
            "|first_name|last_name|       dob|     Fullname|age|\n",
            "+----------+---------+----------+-------------+---+\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|\n",
            "+----------+---------+----------+-------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col, when\n",
        "#2.3\tConditional logic with when() and otherwise()\n",
        "df.withColumn(\"age2\",when(col(\"age\") > 25, \"y\").otherwise(\"n\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzGpIbI7bzoo",
        "outputId": "fe23e3f2-3e4f-474b-b9b2-7d05bdfb9911"
      },
      "id": "xzGpIbI7bzoo",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+----+\n",
            "|first_name|last_name|       dob|     Fullname|age|age2|\n",
            "+----------+---------+----------+-------------+---+----+\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|   y|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|   y|\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|   y|\n",
            "+----------+---------+----------+-------------+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.4\tgroupBy() with aggregation functions (count, avg, sum, max, min)\n",
        "df.groupBy(\"age\").agg({\"age\":\"count\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-CAmORQcBQd",
        "outputId": "a865513a-4d3b-483a-fa9b-e1c48e6cb4c7"
      },
      "id": "X-CAmORQcBQd",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "|age|count(age)|\n",
            "+---+----------+\n",
            "| 29|         3|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.5\tMultiple aggregations in one statement\n",
        "df.groupBy(\"age\").agg({\"age\":\"count\",\"age\":\"avg\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7LqG5UrWZLy",
        "outputId": "21772c35-71fc-465d-f5fe-e2f3c09ee30b"
      },
      "id": "o7LqG5UrWZLy",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "|age|avg(age)|\n",
            "+---+--------+\n",
            "| 29|    29.0|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.6\torderBy() ascending/descending\n",
        "df.orderBy(col(\"first_name\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIcvvWZYcWSq",
        "outputId": "5e5688d2-4263-4818-adf6-91e2682a3f1e"
      },
      "id": "dIcvvWZYcWSq",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+\n",
            "|first_name|last_name|       dob|     Fullname|age|\n",
            "+----------+---------+----------+-------------+---+\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|\n",
            "+----------+---------+----------+-------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.7\tMulti-column ordering\n",
        "df.orderBy([col(\"last_name\").desc(),col(\"first_name\").desc()]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IehWIf50cgve",
        "outputId": "3da99819-534c-4101-8b0f-8a6133da7321"
      },
      "id": "IehWIf50cgve",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+\n",
            "|first_name|last_name|       dob|     Fullname|age|\n",
            "+----------+---------+----------+-------------+---+\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|\n",
            "+----------+---------+----------+-------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.8\tInner, left, right, full joins\n",
        "data1 = [\n",
        "    (1, \"Alice\", \"HR\"),\n",
        "    (2, \"Bob\", \"IT\"),\n",
        "    (3, \"Cathy\", \"Finance\"),\n",
        "    (4, \"David\", \"IT\")\n",
        "]\n",
        "columns1 = [\"emp_id\", \"name\", \"dept\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "df1.show()\n",
        "\n",
        "data2 = [\n",
        "    (1, 5000),\n",
        "    (2, 6000),\n",
        "    (4, 7000),\n",
        "    (5, 8000)\n",
        "]\n",
        "columns2 = [\"emp_id\", \"salary\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "df2.show()\n",
        "\n",
        "print(\"INNER JOIN\\n\")\n",
        "inner_join_df = df1.join(df2, on = \"emp_id\", how = \"inner\")\n",
        "inner_join_df.show()\n",
        "\n",
        "print(\"LEFT JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"left\").show()\n",
        "\n",
        "print(\"RIGHT JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"right\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYhMtM0fcqKn",
        "outputId": "d7b5a757-ef97-440b-8255-716a7c33e1c6"
      },
      "id": "PYhMtM0fcqKn",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+\n",
            "|emp_id| name|   dept|\n",
            "+------+-----+-------+\n",
            "|     1|Alice|     HR|\n",
            "|     2|  Bob|     IT|\n",
            "|     3|Cathy|Finance|\n",
            "|     4|David|     IT|\n",
            "+------+-----+-------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "+------+------+\n",
            "\n",
            "INNER JOIN\n",
            "\n",
            "+------+-----+----+------+\n",
            "|emp_id| name|dept|salary|\n",
            "+------+-----+----+------+\n",
            "|     1|Alice|  HR|  5000|\n",
            "|     2|  Bob|  IT|  6000|\n",
            "|     4|David|  IT|  7000|\n",
            "+------+-----+----+------+\n",
            "\n",
            "LEFT JOIN\n",
            "\n",
            "+------+-----+-------+------+\n",
            "|emp_id| name|   dept|salary|\n",
            "+------+-----+-------+------+\n",
            "|     1|Alice|     HR|  5000|\n",
            "|     2|  Bob|     IT|  6000|\n",
            "|     3|Cathy|Finance|  NULL|\n",
            "|     4|David|     IT|  7000|\n",
            "+------+-----+-------+------+\n",
            "\n",
            "RIGHT JOIN\n",
            "\n",
            "+------+-----+----+------+\n",
            "|emp_id| name|dept|salary|\n",
            "+------+-----+----+------+\n",
            "|     1|Alice|  HR|  5000|\n",
            "|     2|  Bob|  IT|  6000|\n",
            "|     5| NULL|NULL|  8000|\n",
            "|     4|David|  IT|  7000|\n",
            "+------+-----+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.9\tSemi and anti joins\n",
        "print(\"SEMI JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"left_semi\").show()\n",
        "print(\"ANTI JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"left_anti\").show()\n",
        "\n",
        "#2.1\tSelf joins\n",
        "print(\"SELF JOIN\\n\")\n",
        "df1.join(df1, on = \"emp_id\", how = \"inner\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4iW7uytdjKV",
        "outputId": "5e177e93-c1f4-4f40-89fe-a74435864498"
      },
      "id": "t4iW7uytdjKV",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEMI JOIN\n",
            "\n",
            "+------+-----+----+\n",
            "|emp_id| name|dept|\n",
            "+------+-----+----+\n",
            "|     1|Alice|  HR|\n",
            "|     2|  Bob|  IT|\n",
            "|     4|David|  IT|\n",
            "+------+-----+----+\n",
            "\n",
            "ANTI JOIN\n",
            "\n",
            "+------+-----+-------+\n",
            "|emp_id| name|   dept|\n",
            "+------+-----+-------+\n",
            "|     3|Cathy|Finance|\n",
            "+------+-----+-------+\n",
            "\n",
            "SELF JOIN\n",
            "\n",
            "+------+-----+-------+-----+-------+\n",
            "|emp_id| name|   dept| name|   dept|\n",
            "+------+-----+-------+-----+-------+\n",
            "|     1|Alice|     HR|Alice|     HR|\n",
            "|     2|  Bob|     IT|  Bob|     IT|\n",
            "|     3|Cathy|Finance|Cathy|Finance|\n",
            "|     4|David|     IT|David|     IT|\n",
            "+------+-----+-------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [\n",
        "    (1, \"Alice\", \"HR\"),\n",
        "    (2, \"Bob\", \"IT\"),\n",
        "    (3, \"Cathy\", \"Finance\"),\n",
        "    (4, \"David\", \"IT\"),\n",
        "    (5, \"Suther\", \"BYju\")\n",
        "]\n",
        "columns1 = [\"emp_id\", \"name\", \"dept\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "df1.show()\n",
        "\n",
        "data2 = [\n",
        "    (1, 5000),\n",
        "    (2, 6000),\n",
        "    (4, 7000),\n",
        "    (5, 8000),\n",
        "    (6, None)\n",
        "]\n",
        "columns2 = [\"emp_id\", \"salary\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CVc7bXefjMq",
        "outputId": "d19dbec4-f2c8-4ceb-d21e-10f590f12b4f"
      },
      "id": "6CVc7bXefjMq",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+\n",
            "|emp_id|  name|   dept|\n",
            "+------+------+-------+\n",
            "|     1| Alice|     HR|\n",
            "|     2|   Bob|     IT|\n",
            "|     3| Cathy|Finance|\n",
            "|     4| David|     IT|\n",
            "|     5|Suther|   BYju|\n",
            "+------+------+-------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "|     6|  NULL|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.11\tHandling nulls (fillna, dropna, na.replace)\n",
        "df2.fillna({\"salary\":10000}).show()\n",
        "df2.dropna().show()\n",
        "df2.fillna({\"salary\":10000}).na.replace(10000,20000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T70Ct0lkhG3_",
        "outputId": "b4e179a0-4560-4fff-ba8e-649ed8f0310b"
      },
      "id": "T70Ct0lkhG3_",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "|     6| 10000|\n",
            "+------+------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "+------+------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "|     6| 20000|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import equal_null\n",
        "#2.12\tReplace specific values in a column\n",
        "df2.replace(8000,20000).show()\n",
        "\n",
        "df2.na.replace({4:5},subset = [\"salary\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPooOuYohYaw",
        "outputId": "3955dc20-60d1-4951-8103-86148724737e"
      },
      "id": "xPooOuYohYaw",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5| 20000|\n",
            "|     6|  NULL|\n",
            "+------+------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "|     6|  NULL|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.13\tCache and uncache data\n",
        "df2.cache()\n",
        "df2.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0tzTP8HkbAU",
        "outputId": "ac78f701-576a-4275-dd3f-cedda7865c75"
      },
      "id": "_0tzTP8HkbAU",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[emp_id: bigint, salary: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.14\tExplain storage levels\n",
        "df2.storageLevel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W006qGeUkdkJ",
        "outputId": "21f0fe7a-222c-44b1-dac5-841ef803599a"
      },
      "id": "W006qGeUkdkJ",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StorageLevel(False, False, False, False, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    (\"Alice\", \"Math\", 95),\n",
        "    (\"Bob\", \"Math\", 90),\n",
        "    (\"Charlie\", \"Math\", 90),\n",
        "    (\"David\", \"Math\", 85),\n",
        "    (\"Alice\", \"Physics\", 88),\n",
        "    (\"Bob\", \"Physics\", 92),\n",
        "    (\"Charlie\", \"Physics\", 85)\n",
        "]\n",
        "\n",
        "columns = [\"Name\", \"Subject\", \"Score\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jCqsLn-Gu_j",
        "outputId": "eeff0df2-e069-4735-b69d-9cb0cb3f5920"
      },
      "id": "2jCqsLn-Gu_j",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|   Name|Subject|Score|\n",
            "+-------+-------+-----+\n",
            "|  Alice|   Math|   95|\n",
            "|    Bob|   Math|   90|\n",
            "|Charlie|   Math|   90|\n",
            "|  David|   Math|   85|\n",
            "|  Alice|Physics|   88|\n",
            "|    Bob|Physics|   92|\n",
            "|Charlie|Physics|   85|\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1 Ranking functions (rank, dense_rank, row_number)\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, dense_rank, row_number\n",
        "\n",
        "windowSpec = Window.partitionBy(\"Subject\").orderBy(df.Score.desc())"
      ],
      "metadata": {
        "id": "iN5L_38yNpy_"
      },
      "id": "iN5L_38yNpy_",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "just_score_window = Window.orderBy(df.Score.desc())\n",
        "\n",
        "df.withColumn(\"rank\",rank().over(just_score_window)).show()\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(Window.partitionBy(df.Subject).orderBy(df.Score.desc()))).show()\n",
        "df.withColumn(\"row_number\",row_number().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYoamXaBN6wJ",
        "outputId": "6510fa33-59db-4e41-8f42-8c159bf9e16b"
      },
      "id": "JYoamXaBN6wJ",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-----+----+\n",
            "|full_name|Subject|Score|rank|\n",
            "+---------+-------+-----+----+\n",
            "|    Alice|   Math|   95|   1|\n",
            "|      Bob|Physics|   92|   2|\n",
            "|      Bob|   Math|   90|   3|\n",
            "|  Charlie|   Math|   90|   3|\n",
            "|    Alice|Physics|   88|   5|\n",
            "|    David|   Math|   85|   6|\n",
            "|  Charlie|Physics|   85|   6|\n",
            "+---------+-------+-----+----+\n",
            "\n",
            "+---------+-------+-----+----------+\n",
            "|full_name|Subject|Score|dense_rank|\n",
            "+---------+-------+-----+----------+\n",
            "|    Alice|   Math|   95|         1|\n",
            "|      Bob|   Math|   90|         2|\n",
            "|  Charlie|   Math|   90|         2|\n",
            "|    David|   Math|   85|         3|\n",
            "|      Bob|Physics|   92|         1|\n",
            "|    Alice|Physics|   88|         2|\n",
            "|  Charlie|Physics|   85|         3|\n",
            "+---------+-------+-----+----------+\n",
            "\n",
            "+---------+-------+-----+----------+\n",
            "|full_name|Subject|Score|row_number|\n",
            "+---------+-------+-----+----------+\n",
            "|    Alice|   Math|   95|         1|\n",
            "|      Bob|   Math|   90|         2|\n",
            "|  Charlie|   Math|   90|         3|\n",
            "|    David|   Math|   85|         4|\n",
            "|      Bob|Physics|   92|         1|\n",
            "|    Alice|Physics|   88|         2|\n",
            "|  Charlie|Physics|   85|         3|\n",
            "+---------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2\tAggregations over a window (lead, lag, sum over partition)\n",
        "from pyspark.sql.functions import  lag, lead\n",
        "df.withColumn(\"prev_score\", lag(\"Score\").over(windowSpec)).show()\n",
        "df.withColumn(\"next_score\", lead(\"Score\").over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym6C_S91hFFz",
        "outputId": "caf5d2c4-bfcd-4bd8-859f-b653d7c7acd2"
      },
      "id": "Ym6C_S91hFFz",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-----+----------+\n",
            "|full_name|Subject|Score|prev_score|\n",
            "+---------+-------+-----+----------+\n",
            "|    Alice|   Math|   95|      NULL|\n",
            "|      Bob|   Math|   90|        95|\n",
            "|  Charlie|   Math|   90|        90|\n",
            "|    David|   Math|   85|        90|\n",
            "|      Bob|Physics|   92|      NULL|\n",
            "|    Alice|Physics|   88|        92|\n",
            "|  Charlie|Physics|   85|        88|\n",
            "+---------+-------+-----+----------+\n",
            "\n",
            "+---------+-------+-----+----------+\n",
            "|full_name|Subject|Score|next_score|\n",
            "+---------+-------+-----+----------+\n",
            "|    Alice|   Math|   95|        90|\n",
            "|      Bob|   Math|   90|        90|\n",
            "|  Charlie|   Math|   90|        85|\n",
            "|    David|   Math|   85|      NULL|\n",
            "|      Bob|Physics|   92|        88|\n",
            "|    Alice|Physics|   88|        85|\n",
            "|  Charlie|Physics|   85|      NULL|\n",
            "+---------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.3\tBroadcast joins for small datasets\n",
        "from pyspark.sql.functions import broadcast\n",
        "df1.join(broadcast(df2), on = \"emp_id\", how = \"inner\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQRbDmLphq49",
        "outputId": "913edada-06b7-428e-e6bb-a4764802a90a"
      },
      "id": "HQRbDmLphq49",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----+------+\n",
            "|emp_id|  name|dept|salary|\n",
            "+------+------+----+------+\n",
            "|     1| Alice|  HR|  5000|\n",
            "|     2|   Bob|  IT|  6000|\n",
            "|     4| David|  IT|  7000|\n",
            "|     5|Suther|BYju|  8000|\n",
            "+------+------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.4\tCross joins\n",
        "df1.crossJoin(df2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHs51X2NiHCB",
        "outputId": "0f0642de-08cd-463a-c05a-9c5eeca7b800"
      },
      "id": "GHs51X2NiHCB",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+------+\n",
            "|emp_id|  name|   dept|emp_id|salary|\n",
            "+------+------+-------+------+------+\n",
            "|     1| Alice|     HR|     1|  5000|\n",
            "|     1| Alice|     HR|     2|  6000|\n",
            "|     2|   Bob|     IT|     1|  5000|\n",
            "|     2|   Bob|     IT|     2|  6000|\n",
            "|     1| Alice|     HR|     4|  7000|\n",
            "|     1| Alice|     HR|     5|  8000|\n",
            "|     1| Alice|     HR|     6|  NULL|\n",
            "|     2|   Bob|     IT|     4|  7000|\n",
            "|     2|   Bob|     IT|     5|  8000|\n",
            "|     2|   Bob|     IT|     6|  NULL|\n",
            "|     3| Cathy|Finance|     1|  5000|\n",
            "|     3| Cathy|Finance|     2|  6000|\n",
            "|     4| David|     IT|     1|  5000|\n",
            "|     4| David|     IT|     2|  6000|\n",
            "|     5|Suther|   BYju|     1|  5000|\n",
            "|     5|Suther|   BYju|     2|  6000|\n",
            "|     3| Cathy|Finance|     4|  7000|\n",
            "|     3| Cathy|Finance|     5|  8000|\n",
            "|     3| Cathy|Finance|     6|  NULL|\n",
            "|     4| David|     IT|     4|  7000|\n",
            "+------+------+-------+------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHO6Xc9cjdnB",
        "outputId": "bbd48b2b-f4ef-4e86-e4c5-0f5924d9f0a7"
      },
      "id": "nHO6Xc9cjdnB",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-----+\n",
            "|full_name|Subject|Score|\n",
            "+---------+-------+-----+\n",
            "|    Alice|   Math|   95|\n",
            "|      Bob|   Math|   90|\n",
            "|  Charlie|   Math|   90|\n",
            "|    David|   Math|   85|\n",
            "|    Alice|Physics|   88|\n",
            "|      Bob|Physics|   92|\n",
            "|  Charlie|Physics|   85|\n",
            "+---------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.5\tCreate and register Python UDFs\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "double_udf = udf(lambda x:x*2, IntegerType())\n",
        "df.withColumn(\"newCol\",double_udf(\"Score\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnNCzT-5jBEC",
        "outputId": "ca1212fb-83b8-4b2d-fabe-5ffc7ccf1c13"
      },
      "id": "dnNCzT-5jBEC",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-----+------+\n",
            "|full_name|Subject|Score|newCol|\n",
            "+---------+-------+-----+------+\n",
            "|    Alice|   Math|   95|   190|\n",
            "|      Bob|   Math|   90|   180|\n",
            "|  Charlie|   Math|   90|   180|\n",
            "|    David|   Math|   85|   170|\n",
            "|    Alice|Physics|   88|   176|\n",
            "|      Bob|Physics|   92|   184|\n",
            "|  Charlie|Physics|   85|   170|\n",
            "+---------+-------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.7\tExplode arrays and maps\n",
        "from pyspark.sql.functions import explode\n",
        "data = [\n",
        "    (1, [\"a\", \"b\", \"c\"]),\n",
        "    (2, [\"x\", \"y\"])\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"letters\"])\n",
        "df.show()\n",
        "df_exploded = df.withColumn(\"letter\", explode(\"letters\"))\n",
        "df_exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0EnjgGSusdL",
        "outputId": "6934815d-eff0-4850-91c2-ac597ae69a2e"
      },
      "id": "t0EnjgGSusdL",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+\n",
            "| id|  letters|\n",
            "+---+---------+\n",
            "|  1|[a, b, c]|\n",
            "|  2|   [x, y]|\n",
            "+---+---------+\n",
            "\n",
            "+---+---------+------+\n",
            "| id|  letters|letter|\n",
            "+---+---------+------+\n",
            "|  1|[a, b, c]|     a|\n",
            "|  1|[a, b, c]|     b|\n",
            "|  1|[a, b, c]|     c|\n",
            "|  2|   [x, y]|     x|\n",
            "|  2|   [x, y]|     y|\n",
            "+---+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, {\"a\": 10, \"b\": 20}),\n",
        "    (2, {\"x\": 5})\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"scores\"])\n",
        "df.show()\n",
        "df_exploded = df.withColumn((\"key\",\"value\"), explode(\"scores\"))\n",
        "df_exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "UyDa3rOhvZzW",
        "outputId": "e6b1a019-f74c-4b64-8840-1a376b461bbe"
      },
      "id": "UyDa3rOhvZzW",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+\n",
            "| id|            scores|\n",
            "+---+------------------+\n",
            "|  1|{a -> 10, b -> 20}|\n",
            "|  2|          {x -> 5}|\n",
            "+---+------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "An error occurred while calling o1047.withColumn. Trace:\npy4j.Py4JException: Method withColumn([class java.util.ArrayList, class org.apache.spark.sql.Column]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2125638807.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_exploded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_exploded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5172\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m             )\n\u001b[0;32m-> 5174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 raise Py4JError(\n\u001b[0m\u001b[1;32m    331\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o1047.withColumn. Trace:\npy4j.Py4JException: Method withColumn([class java.util.ArrayList, class org.apache.spark.sql.Column]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1\tWrite data with partitioning & bucketing\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"department\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Alice\", \"HR\", 5000),\n",
        "    (2, \"Bob\", \"IT\", 6000),\n",
        "    (3, \"Charlie\", \"IT\", 7000),\n",
        "    (4, \"David\", \"HR\", 5500),\n",
        "    (5, \"Eve\", \"Finance\", 6500),\n",
        "    (6, \"Frank\", \"Finance\", 6000),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TJ_kfMmwXiY",
        "outputId": "2d7b7f89-a63c-4257-d8b0-3e3c70742117"
      },
      "id": "8TJ_kfMmwXiY",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------+------+\n",
            "| id|   name|department|salary|\n",
            "+---+-------+----------+------+\n",
            "|  1|  Alice|        HR|  5000|\n",
            "|  2|    Bob|        IT|  6000|\n",
            "|  3|Charlie|        IT|  7000|\n",
            "|  4|  David|        HR|  5500|\n",
            "|  5|    Eve|   Finance|  6500|\n",
            "|  6|  Frank|   Finance|  6000|\n",
            "+---+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").partitionBy(\"department\").parquet(\"/content/employee_partitioned\")"
      ],
      "metadata": {
        "id": "9fCb1OW_xG3w"
      },
      "id": "9fCb1OW_xG3w",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\") \\\n",
        "    .bucketBy(3, \"department\") \\\n",
        "    .sortBy(\"salary\") \\\n",
        "    .saveAsTable(\"employee_bucketed\")"
      ],
      "metadata": {
        "id": "UTHFj7HCxp9U"
      },
      "id": "UTHFj7HCxp9U",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.explain()"
      ],
      "metadata": {
        "id": "XTKl3P3a1PVW",
        "outputId": "4d7aa1a4-9d9c-435e-e883-0cadb25a6674",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XTKl3P3a1PVW",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Scan ExistingRDD[id#1329,name#1330,department#1331,salary#1332]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:Torch]",
      "language": "python",
      "name": "conda-env-Torch-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}