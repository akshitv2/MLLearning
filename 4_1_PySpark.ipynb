{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-08-10T14:18:55.361917Z",
          "start_time": "2025-08-10T14:18:48.012989Z"
        },
        "id": "initial_id"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from sklearn import datasets\n",
        "import time\n",
        "\n",
        "#1.2 Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SimModeExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "outputs": [],
      "execution_count": 68
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing Configs\n",
        "for k, v in spark.sparkContext.getConf().getAll():\n",
        "    print(f\"{k} = {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEmm_yFKcz0W",
        "outputId": "550de6f4-982b-4ba2-9c7d-51ec7d2f1383"
      },
      "id": "DEmm_yFKcz0W",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.app.id = local-1755073533927\n",
            "spark.executor.id = driver\n",
            "spark.sql.warehouse.dir = file:/content/spark-warehouse\n",
            "spark.app.startTime = 1755073529975\n",
            "spark.rdd.compress = True\n",
            "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.serializer.objectStreamReset = 100\n",
            "spark.app.submitTime = 1755073529594\n",
            "spark.master = local[*]\n",
            "spark.submit.pyFiles = \n",
            "spark.submit.deployMode = client\n",
            "spark.app.name = SimModeExample\n",
            "spark.driver.port = 36121\n",
            "spark.driver.host = b70c4d339e42\n",
            "spark.ui.showConsoleProgress = true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"Apache Spark is an open-source distributed general-purpose cluster-computing framework.\n",
        "It provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\n",
        "Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries, and streaming.\"\"\"\n",
        "\n",
        "with open(\"sample_text.txt\", \"w\") as f:\n",
        "    f.write(sample_text)"
      ],
      "metadata": {
        "id": "XRGAKBTaigSI"
      },
      "id": "XRGAKBTaigSI",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.4 Create RDDs from lists or text files\n",
        "text_rdd = sc.textFile(\"sample_text.txt\")"
      ],
      "metadata": {
        "id": "Z-6wNGwXyGcB"
      },
      "id": "Z-6wNGwXyGcB",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.5\tmap(), filter(), flatMap() transformations\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).collect())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Chw21bytYB",
        "outputId": "5e7dff9c-8509-4e1c-801a-84aaf24ad687"
      },
      "id": "K5Chw21bytYB",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apache', 'Spark', 'is', 'an', 'open-source', 'distributed', 'general-purpose', 'cluster-computing', 'framework.', 'It', 'provides', 'an', 'interface', 'for', 'programming', 'entire', 'clusters', 'with', 'implicit', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'is', 'designed', 'to', 'cover', 'a', 'wide', 'range', 'of', 'workloads', 'such', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'queries,', 'and', 'streaming.']\n",
            "['Apache', 'Spark', 'an', 'general-purpose', 'framework.', 'an', 'interface', 'programming', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'a', 'range', 'workloads', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'and', 'streaming.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.6\tcollect(), count(), take() actions\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).count())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).collect())\n",
        "print(text_rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: \"a\" in x).take(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8MoLveczQ3L",
        "outputId": "6eae0a24-99aa-4db6-dd82-b8a3201f64ea"
      },
      "id": "_8MoLveczQ3L",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "['Apache', 'Spark', 'an', 'general-purpose', 'framework.', 'an', 'interface', 'programming', 'data', 'parallelism', 'and', 'fault-tolerance.', 'Spark', 'a', 'range', 'workloads', 'as', 'batch', 'applications,', 'iterative', 'algorithms,', 'interactive', 'and', 'streaming.']\n",
            "['Apache', 'Spark', 'an']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.7\tCreate DataFrame from Python dictionary/list\n",
        "data_frame_data = [\n",
        "    {\"name\": \"Alice\", \"age\": 25},\n",
        "    {\"name\": \"Bob\", \"age\": 30},\n",
        "    {\"name\": \"Charlie\", \"surname:\":\"theron\",\"age\": 35}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data_frame_data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2Ss9B1X3ekm",
        "outputId": "4ae91d60-1c66-47b7-e43e-c1e24488eae3"
      },
      "id": "y2Ss9B1X3ekm",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+--------+\n",
            "|age|   name|surname:|\n",
            "+---+-------+--------+\n",
            "| 25|  Alice|    NULL|\n",
            "| 30|    Bob|    NULL|\n",
            "| 35|Charlie|  theron|\n",
            "+---+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.8\tCreate DataFrame from CSV/JSON/Parquet\n",
        "csv_data = \"\"\"id,name,age,salary\n",
        "1,Alice,30,100000\n",
        "2,Bob,,85000\n",
        "3,Charlie,25,70000\n",
        "4,David,35,\n",
        "5,Eve,29,90000\n",
        "\"\"\"\n",
        "\n",
        "with open(\"sample_employees.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "df = spark.read.csv(\"sample_employees.csv\", header=True, inferSchema=True)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMzR-9nqI87q",
        "outputId": "1497a378-4369-406b-b8ee-3f3f870912b0"
      },
      "id": "ZMzR-9nqI87q",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----+------+\n",
            "| id|   name| age|salary|\n",
            "+---+-------+----+------+\n",
            "|  1|  Alice|  30|100000|\n",
            "|  2|    Bob|NULL| 85000|\n",
            "|  3|Charlie|  25| 70000|\n",
            "|  4|  David|  35|  NULL|\n",
            "|  5|    Eve|  29| 90000|\n",
            "+---+-------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.9\tShow schema and data (printSchema(), show())\n",
        "print(df.printSchema())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl4bZJZl5lkD",
        "outputId": "07a6d055-82a5-47d4-a339-51dfca22adf5"
      },
      "id": "vl4bZJZl5lkD",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.10\tSelect specific columns\n",
        "df.select(\"name\",\"age\").show()"
      ],
      "metadata": {
        "id": "g7en7TvM6f7B",
        "outputId": "ab089eff-ac99-4a53-805c-07e17599eab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "g7en7TvM6f7B",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|   name| age|\n",
            "+-------+----+\n",
            "|  Alice|  30|\n",
            "|    Bob|NULL|\n",
            "|Charlie|  25|\n",
            "|  David|  35|\n",
            "|    Eve|  29|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.11\tFilter rows using conditions\n",
        "test_lambda_func = lambda x: x-2\n",
        "df.select(\"name\",\"age\").filter(test_lambda_func(df.age)>25).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkfyjPy75v4g",
        "outputId": "240a6fcb-f436-4ef0-96a5-061fa7e4a245"
      },
      "id": "GkfyjPy75v4g",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 30|\n",
            "|David| 35|\n",
            "|  Eve| 29|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.12\tRename columns\n",
        "df = df.withColumnRenamed(\"name\", \"full_name\")\n",
        "df.printSchema()\n",
        "\n",
        "new_column_names = [\"user_id\", \"full_name\",\"age\", \"salary net\"]\n",
        "df_renamed = df.toDF(*new_column_names)\n",
        "df_renamed.printSchema()"
      ],
      "metadata": {
        "id": "mtTNZdip6lJK",
        "outputId": "6f65ecfc-4857-43c0-f53e-3685a61267ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mtTNZdip6lJK",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary net: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.13\tAdd new columns (withColumn)\n",
        "df = df.withColumn(\"age2\", df.age+2)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "kC_AnNCH8vIe",
        "outputId": "e1d55224-6e33-452b-c27a-a56a7fc0da8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kC_AnNCH8vIe",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- age2: integer (nullable = true)\n",
            "\n",
            "+---+---------+----+------+----+\n",
            "| id|full_name| age|salary|age2|\n",
            "+---+---------+----+------+----+\n",
            "|  1|    Alice|  30|100000|  32|\n",
            "|  2|      Bob|NULL| 85000|NULL|\n",
            "|  3|  Charlie|  25| 70000|  27|\n",
            "|  4|    David|  35|  NULL|  37|\n",
            "|  5|      Eve|  29| 90000|  31|\n",
            "+---+---------+----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.14\tDrop columns\n",
        "df = df.drop(\"age2\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "5NfoH37y9GJ6",
        "outputId": "2cc48411-4952-44e2-b24c-e1ac8e0fa4c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5NfoH37y9GJ6",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- full_name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.15\tRegister DataFrame as a SQL temporary view\n",
        "#1.16\tRun simple SELECT queries with spark.sql()\n",
        "\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "result = spark.sql(\"SELECT full_name, age FROM people WHERE age > 26\")\n",
        "result.show()\n",
        "\n",
        "# createOrReplaceTempView: Visible only within the current Spark session\n",
        "# createGlobalTempView   : Visible across multiple Spark sessions and accessed with the prefix global_temp."
      ],
      "metadata": {
        "id": "YuQudJGE9KFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37814e50-f262-4799-b5fa-ba916e9ce54b"
      },
      "id": "YuQudJGE9KFO",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|full_name|age|\n",
            "+---------+---+\n",
            "|    Alice| 30|\n",
            "|    David| 35|\n",
            "|      Eve| 29|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat, upper, lower, trim, col, lit\n",
        "#New Sample DF\n",
        "\n",
        "data = [(\" Alice \", \"Smith\",\"01-01-1996\"), (\" Bob \", \"Brown\",\"01-01-1996\"), (\" Cathy \", \"Johnson\",\"01-01-1996\")]\n",
        "columns = [\"first_name\", \"last_name\", \"dob\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "#2.1\tString functions (concat, upper, lower, trim)\n",
        "df = df.withColumn(\"first_name\", trim(col(\"first_name\")))\n",
        "df = df.withColumn(\"last_name\", trim(col(\"last_name\")))\n",
        "df = df.withColumn(\"Fullname\", concat(col(\"first_name\"),lit(\" \"),\"last_name\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUMm4GOqIe9G",
        "outputId": "2e56df5d-6806-4c72-d49b-14a4d8e2a81f"
      },
      "id": "kUMm4GOqIe9G",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+\n",
            "|first_name|last_name|       dob|\n",
            "+----------+---------+----------+\n",
            "|    Alice |    Smith|01-01-1996|\n",
            "|      Bob |    Brown|01-01-1996|\n",
            "|    Cathy |  Johnson|01-01-1996|\n",
            "+----------+---------+----------+\n",
            "\n",
            "+----------+---------+----------+-------------+\n",
            "|first_name|last_name|       dob|     Fullname|\n",
            "+----------+---------+----------+-------------+\n",
            "|     Alice|    Smith|01-01-1996|  Alice Smith|\n",
            "|       Bob|    Brown|01-01-1996|    Bob Brown|\n",
            "|     Cathy|  Johnson|01-01-1996|Cathy Johnson|\n",
            "+----------+---------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col, datediff, current_date, floor\n",
        "#2.2\tDate/time functions (current_date, datediff, date_format)\n",
        "df = df.withColumn(\"dob\", to_date(col(\"dob\"), format= \"dd-mm-yyyy\"))\n",
        "df = df.withColumn(\"age\", floor(datediff(current_date(), col(\"dob\"))/365))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJlxAciIVU4q",
        "outputId": "de83452a-6722-4a39-e0c5-7c3cb91f1b24"
      },
      "id": "HJlxAciIVU4q",
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+\n",
            "|first_name|last_name|       dob|     Fullname|age|\n",
            "+----------+---------+----------+-------------+---+\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|\n",
            "+----------+---------+----------+-------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col, when\n",
        "#2.3\tConditional logic with when() and otherwise()\n",
        "df.withColumn(\"age2\",when(col(\"age\") > 25, \"y\").otherwise(\"n\")).show()"
      ],
      "metadata": {
        "id": "xzGpIbI7bzoo",
        "outputId": "ae8f3fbe-9731-488c-9fca-495f424ecf6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xzGpIbI7bzoo",
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+----+\n",
            "|first_name|last_name|       dob|     Fullname|age|age2|\n",
            "+----------+---------+----------+-------------+---+----+\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|   y|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|   y|\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|   y|\n",
            "+----------+---------+----------+-------------+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.4\tgroupBy() with aggregation functions (count, avg, sum, max, min)\n",
        "df.groupBy(\"age\").agg({\"age\":\"count\"}).show()"
      ],
      "metadata": {
        "id": "X-CAmORQcBQd",
        "outputId": "37faafcf-ed45-487a-922b-f96b4c1f9085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "X-CAmORQcBQd",
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "|age|count(age)|\n",
            "+---+----------+\n",
            "| 29|         3|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.5\tMultiple aggregations in one statement\n",
        "df.groupBy(\"age\").agg({\"age\":\"count\",\"age\":\"avg\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7LqG5UrWZLy",
        "outputId": "00a55be3-e868-497a-deeb-afdc957e4d98"
      },
      "id": "o7LqG5UrWZLy",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "|age|avg(age)|\n",
            "+---+--------+\n",
            "| 29|    29.0|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.6\torderBy() ascending/descending\n",
        "df.orderBy(col(\"first_name\").desc()).show()"
      ],
      "metadata": {
        "id": "dIcvvWZYcWSq",
        "outputId": "a50d7ae3-bcf6-41a2-d875-dde9aa95b33a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dIcvvWZYcWSq",
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+\n",
            "|first_name|last_name|       dob|     Fullname|age|\n",
            "+----------+---------+----------+-------------+---+\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|\n",
            "+----------+---------+----------+-------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.7\tMulti-column ordering\n",
        "df.orderBy([col(\"last_name\").desc(),col(\"first_name\").desc()]).show()"
      ],
      "metadata": {
        "id": "IehWIf50cgve",
        "outputId": "97533d85-bc84-4a02-db1d-54d70e79141e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IehWIf50cgve",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+-------------+---+\n",
            "|first_name|last_name|       dob|     Fullname|age|\n",
            "+----------+---------+----------+-------------+---+\n",
            "|     Alice|    Smith|1996-01-01|  Alice Smith| 29|\n",
            "|     Cathy|  Johnson|1996-01-01|Cathy Johnson| 29|\n",
            "|       Bob|    Brown|1996-01-01|    Bob Brown| 29|\n",
            "+----------+---------+----------+-------------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.8\tInner, left, right, full joins\n",
        "data1 = [\n",
        "    (1, \"Alice\", \"HR\"),\n",
        "    (2, \"Bob\", \"IT\"),\n",
        "    (3, \"Cathy\", \"Finance\"),\n",
        "    (4, \"David\", \"IT\")\n",
        "]\n",
        "columns1 = [\"emp_id\", \"name\", \"dept\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "df1.show()\n",
        "\n",
        "data2 = [\n",
        "    (1, 5000),\n",
        "    (2, 6000),\n",
        "    (4, 7000),\n",
        "    (5, 8000)\n",
        "]\n",
        "columns2 = [\"emp_id\", \"salary\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "df2.show()\n",
        "\n",
        "print(\"INNER JOIN\\n\")\n",
        "inner_join_df = df1.join(df2, on = \"emp_id\", how = \"inner\")\n",
        "inner_join_df.show()\n",
        "\n",
        "print(\"LEFT JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"left\").show()\n",
        "\n",
        "print(\"RIGHT JOIN\\n\")\n",
        "df1.join(df2, on = \"emp_id\", how = \"right\").show()"
      ],
      "metadata": {
        "id": "PYhMtM0fcqKn",
        "outputId": "14d2d0f5-874f-4010-a2a6-b831eef00232",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PYhMtM0fcqKn",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+\n",
            "|emp_id| name|   dept|\n",
            "+------+-----+-------+\n",
            "|     1|Alice|     HR|\n",
            "|     2|  Bob|     IT|\n",
            "|     3|Cathy|Finance|\n",
            "|     4|David|     IT|\n",
            "+------+-----+-------+\n",
            "\n",
            "+------+------+\n",
            "|emp_id|salary|\n",
            "+------+------+\n",
            "|     1|  5000|\n",
            "|     2|  6000|\n",
            "|     4|  7000|\n",
            "|     5|  8000|\n",
            "+------+------+\n",
            "\n",
            "INNER JOIN\n",
            "\n",
            "+------+-----+----+------+\n",
            "|emp_id| name|dept|salary|\n",
            "+------+-----+----+------+\n",
            "|     1|Alice|  HR|  5000|\n",
            "|     2|  Bob|  IT|  6000|\n",
            "|     4|David|  IT|  7000|\n",
            "+------+-----+----+------+\n",
            "\n",
            "LEFT JOIN\n",
            "\n",
            "+------+-----+-------+------+\n",
            "|emp_id| name|   dept|salary|\n",
            "+------+-----+-------+------+\n",
            "|     1|Alice|     HR|  5000|\n",
            "|     2|  Bob|     IT|  6000|\n",
            "|     3|Cathy|Finance|  NULL|\n",
            "|     4|David|     IT|  7000|\n",
            "+------+-----+-------+------+\n",
            "\n",
            "RIGHT JOIN\n",
            "\n",
            "+------+-----+----+------+\n",
            "|emp_id| name|dept|salary|\n",
            "+------+-----+----+------+\n",
            "|     1|Alice|  HR|  5000|\n",
            "|     2|  Bob|  IT|  6000|\n",
            "|     5| NULL|NULL|  8000|\n",
            "|     4|David|  IT|  7000|\n",
            "+------+-----+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.9\tSemi and anti joins\n"
      ],
      "metadata": {
        "id": "t4iW7uytdjKV"
      },
      "id": "t4iW7uytdjKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load text file into RDD\n",
        "text_rdd = sc.textFile(\"sample_text.txt\")\n",
        "\n",
        "def top_words_no_cache():\n",
        "    word_counts = (\n",
        "        text_rdd\n",
        "        .flatMap(lambda line: line.split())\n",
        "        .map(lambda w: (w.lower().strip(\".,!?\"), 1))\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "    )\n",
        "    top_10 = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
        "    return top_10\n",
        "\n",
        "def top_words_with_cache():\n",
        "    cached_rdd = (\n",
        "        text_rdd\n",
        "        .flatMap(lambda line: line.split())\n",
        "        .map(lambda w: (w.lower().strip(\".,!?\"), 1))\n",
        "        .cache()  # Cache the intermediate RDD\n",
        "    )\n",
        "    word_counts = cached_rdd.reduceByKey(lambda a, b: a + b)\n",
        "    top_10 = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
        "    return top_10\n",
        "\n",
        "# Measure time without caching\n",
        "start = time.time()\n",
        "result_no_cache = top_words_no_cache()\n",
        "time_no_cache = time.time() - start\n",
        "\n",
        "# Measure time with caching\n",
        "start = time.time()\n",
        "result_with_cache = top_words_with_cache()\n",
        "time_with_cache = time.time() - start\n",
        "\n",
        "# Print results\n",
        "print(\"\\nTop 10 words without caching:\")\n",
        "for word, count in result_no_cache:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\nTime taken without caching: {time_no_cache:.4f} seconds\")\n",
        "\n",
        "print(\"\\nTop 10 words with caching:\")\n",
        "for word, count in result_with_cache:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\nTime taken with caching: {time_with_cache:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTjmZLkKmvty",
        "outputId": "5c314a9a-9b96-489b-8245-d8f71b2636ca"
      },
      "id": "PTjmZLkKmvty",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 words without caching:\n",
            "an: 2\n",
            "and: 2\n",
            "spark: 2\n",
            "is: 2\n",
            "apache: 1\n",
            "open-source: 1\n",
            "distributed: 1\n",
            "cluster-computing: 1\n",
            "framework: 1\n",
            "it: 1\n",
            "\n",
            "Time taken without caching: 1.1633 seconds\n",
            "\n",
            "Top 10 words with caching:\n",
            "an: 2\n",
            "and: 2\n",
            "spark: 2\n",
            "is: 2\n",
            "apache: 1\n",
            "open-source: 1\n",
            "distributed: 1\n",
            "cluster-computing: 1\n",
            "framework: 1\n",
            "it: 1\n",
            "\n",
            "Time taken with caching: 1.3649 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Map Reduce Word Frequency\n",
        "text_rdd = spark.sparkContext.parallelize([\n",
        "    \"PySpark is great\",\n",
        "    \"PySpark runs locally\",\n",
        "    \"Word count is a classic example\"\n",
        "])\n",
        "\n",
        "# Word count\n",
        "word_counts = (\n",
        "    text_rdd\n",
        "    .flatMap(lambda line: line.split(\" \"))\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "print(\"\\n=== Word Count ===\")\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3pL2GW-iDNC",
        "outputId": "9fb060ca-3fdd-4a74-b066-22a1a16b9c2d"
      },
      "id": "e3pL2GW-iDNC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Word Count ===\n",
            "PySpark: 2\n",
            "runs: 1\n",
            "Word: 1\n",
            "is: 2\n",
            "great: 1\n",
            "locally: 1\n",
            "count: 1\n",
            "a: 1\n",
            "classic: 1\n",
            "example: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd = sc.textFile(\"sample_text.txt\")\n",
        "text_rdd.map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-nNprHbiMgT",
        "outputId": "7b1a062c-7be8-4049-cadb-f2e205aa65cd"
      },
      "id": "K-nNprHbiMgT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.',\n",
              "  1),\n",
              " ('Apache Spark is an open-source distributed general-purpose cluster-computing framework.',\n",
              "  1),\n",
              " ('Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries, and streaming.',\n",
              "  1)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd.flatMap(lambda sent: sent.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a,b:a+b).collect()"
      ],
      "metadata": {
        "id": "6VHdKuWNijsZ"
      },
      "id": "6VHdKuWNijsZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd.flatMap(lambda sent: sent.split(\" \")).filter(lambda x: \"a\" in x).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCXZ8EDSlZWj",
        "outputId": "197fad45-172c-46b8-9249-a88aff5df1df"
      },
      "id": "PCXZ8EDSlZWj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apache',\n",
              " 'Spark',\n",
              " 'an',\n",
              " 'general-purpose',\n",
              " 'framework.',\n",
              " 'an',\n",
              " 'interface',\n",
              " 'programming',\n",
              " 'data',\n",
              " 'parallelism',\n",
              " 'and',\n",
              " 'fault-tolerance.',\n",
              " 'Spark',\n",
              " 'a',\n",
              " 'range',\n",
              " 'workloads',\n",
              " 'as',\n",
              " 'batch',\n",
              " 'applications,',\n",
              " 'iterative',\n",
              " 'algorithms,',\n",
              " 'interactive',\n",
              " 'and',\n",
              " 'streaming.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"sample_employees.csv\", header=True, inferSchema=True)\n",
        "\n",
        "print(\"Original Data:\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TW4q7szJH6Y",
        "outputId": "6333dfab-2f12-431d-8992-414bb3458c23"
      },
      "id": "8TW4q7szJH6Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "+---+-------+----+------+\n",
            "| id|   name| age|salary|\n",
            "+---+-------+----+------+\n",
            "|  1|  Alice|  30|100000|\n",
            "|  2|    Bob|NULL| 85000|\n",
            "|  3|Charlie|  25| 70000|\n",
            "|  4|  David|  35|  NULL|\n",
            "|  5|    Eve|  29| 90000|\n",
            "+---+-------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-10T14:19:28.924529Z",
          "start_time": "2025-08-10T14:19:25.057372Z"
        },
        "id": "1cbe3d220d7a182d"
      },
      "cell_type": "code",
      "source": [
        "dataset_iris = datasets.load_iris()\n",
        "spdf = spark.createDataFrame(dataset_iris.data, schema = dataset_iris.feature_names)"
      ],
      "id": "1cbe3d220d7a182d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-10T14:19:51.369672Z",
          "start_time": "2025-08-10T14:19:33.233690Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2b8261243ddb6d",
        "outputId": "287c4ede-a180-4caf-82d9-631a55871542"
      },
      "cell_type": "code",
      "source": [
        "spdf.head(5)"
      ],
      "id": "ae2b8261243ddb6d",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(sepal length (cm)=5.1, sepal width (cm)=3.5, petal length (cm)=1.4, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=4.9, sepal width (cm)=3.0, petal length (cm)=1.4, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=4.7, sepal width (cm)=3.2, petal length (cm)=1.3, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=4.6, sepal width (cm)=3.1, petal length (cm)=1.5, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=5.0, sepal width (cm)=3.6, petal length (cm)=1.4, petal width (cm)=0.2)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "564503e2bcfc66fa"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "564503e2bcfc66fa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:Torch]",
      "language": "python",
      "name": "conda-env-Torch-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}