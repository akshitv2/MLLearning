{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-08-10T14:18:55.361917Z",
          "start_time": "2025-08-10T14:18:48.012989Z"
        },
        "id": "initial_id"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from sklearn import datasets\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SimModeExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing Configs\n",
        "for k, v in spark.sparkContext.getConf().getAll():\n",
        "    print(f\"{k} = {v}\")"
      ],
      "metadata": {
        "id": "DEmm_yFKcz0W",
        "outputId": "ddbc09e0-a5ed-4365-f158-4b4f9f79a677",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DEmm_yFKcz0W",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.driver.port = 43483\n",
            "spark.executor.id = driver\n",
            "spark.sql.warehouse.dir = file:/content/spark-warehouse\n",
            "spark.driver.host = 48d31d4fb928\n",
            "spark.app.submitTime = 1754835684671\n",
            "spark.app.startTime = 1754835684981\n",
            "spark.rdd.compress = True\n",
            "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.serializer.objectStreamReset = 100\n",
            "spark.master = local[*]\n",
            "spark.submit.pyFiles = \n",
            "spark.submit.deployMode = client\n",
            "spark.app.name = SimModeExample\n",
            "spark.ui.showConsoleProgress = true\n",
            "spark.app.id = local-1754835687349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Map Reduce Word Frequency\n",
        "text_rdd = spark.sparkContext.parallelize([\n",
        "    \"PySpark is great\",\n",
        "    \"PySpark runs locally\",\n",
        "    \"Word count is a classic example\"\n",
        "])\n",
        "\n",
        "# Word count\n",
        "word_counts = (\n",
        "    text_rdd\n",
        "    .flatMap(lambda line: line.split(\" \"))\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "print(\"\\n=== Word Count ===\")\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "e3pL2GW-iDNC",
        "outputId": "9fb060ca-3fdd-4a74-b066-22a1a16b9c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e3pL2GW-iDNC",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Word Count ===\n",
            "PySpark: 2\n",
            "runs: 1\n",
            "Word: 1\n",
            "is: 2\n",
            "great: 1\n",
            "locally: 1\n",
            "count: 1\n",
            "a: 1\n",
            "classic: 1\n",
            "example: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"Apache Spark is an open-source distributed general-purpose cluster-computing framework.\n",
        "It provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\n",
        "Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries, and streaming.\"\"\"\n",
        "\n",
        "with open(\"sample_text.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Load text file into RDD\n",
        "text_rdd = sc.textFile(\"sample_text.txt\")\n",
        "\n",
        "def top_words_no_cache():\n",
        "    word_counts = (\n",
        "        text_rdd\n",
        "        .flatMap(lambda line: line.split())\n",
        "        .map(lambda w: (w.lower().strip(\".,!?\"), 1))\n",
        "        .reduceByKey(lambda a, b: a + b)\n",
        "    )\n",
        "    top_10 = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
        "    return top_10\n",
        "\n",
        "def top_words_with_cache():\n",
        "    cached_rdd = (\n",
        "        text_rdd\n",
        "        .flatMap(lambda line: line.split())\n",
        "        .map(lambda w: (w.lower().strip(\".,!?\"), 1))\n",
        "        .cache()  # Cache the intermediate RDD\n",
        "    )\n",
        "    word_counts = cached_rdd.reduceByKey(lambda a, b: a + b)\n",
        "    top_10 = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
        "    return top_10\n",
        "\n",
        "# Measure time without caching\n",
        "start = time.time()\n",
        "result_no_cache = top_words_no_cache()\n",
        "time_no_cache = time.time() - start\n",
        "\n",
        "# Measure time with caching\n",
        "start = time.time()\n",
        "result_with_cache = top_words_with_cache()\n",
        "time_with_cache = time.time() - start\n",
        "\n",
        "# Print results\n",
        "print(\"\\nTop 10 words without caching:\")\n",
        "for word, count in result_no_cache:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\nTime taken without caching: {time_no_cache:.4f} seconds\")\n",
        "\n",
        "print(\"\\nTop 10 words with caching:\")\n",
        "for word, count in result_with_cache:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\nTime taken with caching: {time_with_cache:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "PTjmZLkKmvty",
        "outputId": "5c314a9a-9b96-489b-8245-d8f71b2636ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PTjmZLkKmvty",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 words without caching:\n",
            "an: 2\n",
            "and: 2\n",
            "spark: 2\n",
            "is: 2\n",
            "apache: 1\n",
            "open-source: 1\n",
            "distributed: 1\n",
            "cluster-computing: 1\n",
            "framework: 1\n",
            "it: 1\n",
            "\n",
            "Time taken without caching: 1.1633 seconds\n",
            "\n",
            "Top 10 words with caching:\n",
            "an: 2\n",
            "and: 2\n",
            "spark: 2\n",
            "is: 2\n",
            "apache: 1\n",
            "open-source: 1\n",
            "distributed: 1\n",
            "cluster-computing: 1\n",
            "framework: 1\n",
            "it: 1\n",
            "\n",
            "Time taken with caching: 1.3649 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixhVDXDrm5ed"
      },
      "id": "ixhVDXDrm5ed",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_rdd.flatMap(lambda line: line.split(\" \")))"
      ],
      "metadata": {
        "id": "pRI8bmnRk83H",
        "outputId": "5b7bf1e3-d005-4c03-e246-2a17c66fd3f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pRI8bmnRk83H",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[27] at RDD at PythonRDD.scala:53\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-10T14:19:28.924529Z",
          "start_time": "2025-08-10T14:19:25.057372Z"
        },
        "id": "1cbe3d220d7a182d"
      },
      "cell_type": "code",
      "source": [
        "dataset_iris = datasets.load_iris()\n",
        "spdf = spark.createDataFrame(dataset_iris.data, schema = dataset_iris.feature_names)"
      ],
      "id": "1cbe3d220d7a182d",
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-10T14:19:51.369672Z",
          "start_time": "2025-08-10T14:19:33.233690Z"
        },
        "id": "ae2b8261243ddb6d",
        "outputId": "287c4ede-a180-4caf-82d9-631a55871542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "spdf.head(5)"
      ],
      "id": "ae2b8261243ddb6d",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(sepal length (cm)=5.1, sepal width (cm)=3.5, petal length (cm)=1.4, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=4.9, sepal width (cm)=3.0, petal length (cm)=1.4, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=4.7, sepal width (cm)=3.2, petal length (cm)=1.3, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=4.6, sepal width (cm)=3.1, petal length (cm)=1.5, petal width (cm)=0.2),\n",
              " Row(sepal length (cm)=5.0, sepal width (cm)=3.6, petal length (cm)=1.4, petal width (cm)=0.2)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "564503e2bcfc66fa"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "564503e2bcfc66fa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:Torch]",
      "language": "python",
      "name": "conda-env-Torch-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}