---
title: Deep Learning Recall
nav_order: 5
parent: Recall
layout: default
---

1. What are the main reasons data cleaning is necessary in data preprocessing, and can you provide examples of inconsistent data formats that might require cleaning?
2. Describe the different techniques for handling missing data in a dataset. When would you choose to remove samples versus using imputation methods?
3. Explain the Z-score method for outlier detection. How does the Central Limit Theorem relate to identifying outliers with a Z-score threshold of |z| > 3?
4. Compare and contrast the IQR method and Mahalanobis Distance for outlier detection. In what scenarios would you prefer one over the other?
5. What is the difference between standardization and min-max scaling in data transformation? Provide a use case where each would be appropriate.
6. How does the Box-Cox transform stabilize variance in a dataset, and what are its limitations when applied to non-positive data?
7. When handling outliers, how do you decide whether to remove them, replace them, or keep them? Provide an example where keeping outliers is beneficial.
8. Discuss the pros and cons of one-hot encoding versus label encoding for categorical variables. When would you use ordinal encoding instead?
9. Explain how target encoding works for categorical variables. Why is it risky to use this method, and how can target leakage be mitigated?
10. What is multicollinearity, and how does it impact machine learning models? Describe how you would use a correlation matrix to identify and address it.
11. How does Principal Component Analysis (PCA) differ from t-SNE in dimensionality reduction? When would you choose t-SNE over PCA for a project?
12. Compare stratified sampling and cluster sampling in probability sampling. Provide a scenario where one would be more suitable than the other.
13. What are the risks of using convenience sampling in non-probability sampling, and how might these affect the validity of a study?
14. Explain how SMOTE works to address class imbalance. What are its potential drawbacks, and how does ADASYN improve upon it?
15. In text preprocessing, what is the difference between stemming and lemmatization? Why might lemmatization be preferred in natural language processing tasks?
16. Describe the Bag of Words (BoW) and TF-IDF methods for text vectorization. How do their approaches to representing text differ?
17. What are the limitations of static word embeddings like Word2Vec compared to dynamic embeddings like BERT? Provide an example where dynamic embeddings are critical.
18. How does the Skip-Gram model in Word2Vec differ from the Continuous Bag of Words (CBOW) model? In what cases would Skip-Gram perform better?
19. Explain how Item2Vec can be used to generate recommendations for non-textual data, such as movies or products. Provide a practical example.
20. What is the role of subword tokenization in modern NLP models, and how do Byte Pair Encoding (BPE) and WordPiece Encoding differ in their approach?